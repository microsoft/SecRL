{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'executionTime': 0.0156209, 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 35587, 'missbytes': 0, 'retrievebytes': 0}, 'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0}, 'bypassbytes': 0}}, 'cpu': {'user': '00:00:00', 'kernel': '00:00:00', 'totalCpu': '00:00:00', 'breakdown': {'queryExecution': '00:00:00', 'queryPlanning': '00:00:00'}}, 'memory': {'peakPerNode': 65546752}, 'network': {'interClusterTotalBytes': 12882, 'crossClusterTotalBytes': 0}}, 'inputDatasetStatistics': {'extents': {'total': 68, 'scanned': 1, 'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z', 'scannedMaxDatetime': '2024-06-18T00:00:00.0000000Z'}, 'rows': {'total': 43242, 'scanned': 215}, 'rowstores': {'scannedRows': 611, 'scannedValuesSize': 2061095}, 'shards': {'queriesGeneric': 1, 'queriesSpecialized': 0}}, 'datasetStatistics': [{'tableRowCount': 1, 'tableSize': 603}], 'crossClusterResourceUsage': {}}}\n",
      "<azure.monitor.query._models.LogsQueryResult object at 0x000001F54C782B90>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_data(\n",
    "        query, \n",
    "        \n",
    "        timpespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]]):\n",
    "    response = client.query_workspace(Alpine, query, timespan=timpespan, include_statistics=True)\n",
    "    print(response.statistics)\n",
    "    # get total bytes processed\n",
    "    # print(response.statistics[\"network\"]['interClusterTotalBytes'])\n",
    "    return response\n",
    "    if response.status == LogsQueryStatus.SUCCESS:\n",
    "        return response.tables\n",
    "    else:\n",
    "        return response.partial_data\n",
    "    \n",
    "    \n",
    "# 45 days\n",
    "start_time = datetime(2024, 6, 18, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "end_time = datetime(2024, 8, 2, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# for 3 hours\n",
    "duration = timedelta(hours=3)\n",
    "# end_time = start_time + duration\n",
    "\n",
    "resutl = query_data(\"AADManagedIdentitySignInLogs\", (start_time, duration))\n",
    "print(resutl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import timedelta, datetime, timezone\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from typing import Optional, Union, Tuple\n",
    "from textwrap import dedent\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = LogsQueryClient(credential)\n",
    "def get_count(\n",
    "        workspace_id: str,\n",
    "        table_name: str,\n",
    "        timpespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]]\n",
    "    ) -> int:\n",
    "\n",
    "    response = client.query_workspace(workspace_id, f\"{table_name} | count\", timespan=timpespan)\n",
    "\n",
    "    if response.status == LogsQueryStatus.SUCCESS:\n",
    "        return response.tables[0].rows[0][0]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def calculate_size_per_entry(\n",
    "        workspace_id: str,\n",
    "        table_name: str,\n",
    "        timpespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]]\n",
    "    ) -> int:\n",
    "\n",
    "    response = client.query_workspace(workspace_id, f\"{table_name} | limit 2000\", timespan=timpespan, include_statistics=True)\n",
    "    if response.status == LogsQueryStatus.SUCCESS:\n",
    "        if  response.statistics['query']['datasetStatistics'][0]['tableRowCount'] == 0:\n",
    "            print(f\"Table {table_name} is empty.\")\n",
    "            return 0\n",
    "        return response.statistics['query']['datasetStatistics'][0]['tableSize'] / response.statistics['query']['datasetStatistics'][0]['tableRowCount']\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def check_segemented_query(\n",
    "        workspace_id: str,\n",
    "        table_name: str,\n",
    "        timespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]],\n",
    "        max_size_allowed: int = 60000000, # in bytes, ~60MB\n",
    "        max_count_allowed: int = 500000,\n",
    "        verbose: bool = False\n",
    "    ) -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Check if the query need to be segmented or not\n",
    "    \n",
    "    Args:\n",
    "    - workspace_id: Azure Log Analytics workspace id\n",
    "    - table_name: table name to query\n",
    "    - timespan: time range to query\n",
    "    - max_size_allowed: maximum size allowed for a single query\n",
    "    - max_count_allowed: maximum count allowed for a single query\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[bool, int, int, int]:\n",
    "        - bool: if the query need to be segmented or not\n",
    "        - int: number of row per query\n",
    "        - int: total count of the table\n",
    "        - int: total size of the table, in bytes\n",
    "    \"\"\"\n",
    "    # https://learn.microsoft.com/en-us/azure/azure-monitor/service-limits#la-query-api\n",
    "\n",
    "    total_count = get_count(workspace_id, table_name, timespan)\n",
    "    size_per_entry = calculate_size_per_entry(workspace_id, table_name, timespan)\n",
    "    if total_count == -1 or size_per_entry == -1:\n",
    "        return True, 0, -1, -1\n",
    "    total_size = total_count * size_per_entry \n",
    "    if total_size < max_size_allowed and total_count < max_count_allowed:\n",
    "        if verbose:\n",
    "            print(f\"Total count: {total_count}\", f\"Size per entry: {size_per_entry}\", f\"Total size: {total_size/1024/1024} MB\")\n",
    "        return False, -1, total_count, total_size\n",
    "    \n",
    "    row_per_query = max_count_allowed\n",
    "    if total_size > max_size_allowed:\n",
    "        # get number of row per query \n",
    "        row_per_query = min(row_per_query, max_size_allowed // size_per_entry)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Table: {table_name}\", \n",
    "              f\"Total count: {total_count}\", \n",
    "              f\"Size per entry: {size_per_entry} Bytes\", \n",
    "              f\"Total size: {total_size/1024/1024} MB\", \n",
    "              f\"Row per query: {row_per_query}\", \n",
    "              f\"Estimate table count: {total_count // row_per_query}\",\n",
    "              sep=\", \",\n",
    "              )\n",
    "    return True, int(row_per_query), total_count, total_size\n",
    "\n",
    "def query_and_save_data(\n",
    "        workspace_id: str,\n",
    "        table_name: str,\n",
    "        timespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]],\n",
    "        file_path: str,\n",
    "        verbose: bool = False\n",
    "):\n",
    "    \n",
    "    def save_table(file_path_name, response, need_metadata=False):\n",
    "        table = response.tables[0]\n",
    "        df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
    "        if len(df) == 0:\n",
    "            print(f\"Table {table_name} is empty. Skipping.\")\n",
    "            return -1, -1\n",
    "        metadata = dict(zip(df.columns.tolist(), table.columns_types))\n",
    "        if need_metadata:\n",
    "            with open(file_path_name.replace(\".csv\", \"_metadata.json\"), \"w\") as f:\n",
    "                json.dump(metadata, f)\n",
    "\n",
    "        json_columns = [col for col, dtype in metadata.items() if dtype == \"dynamic\"]\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(lambda x: \"{}\" if x == \"\" else x)\n",
    "            df[column] = df[column].apply(lambda x: \"{}\" if pd.isnull(x) else x)\n",
    "        \n",
    "        df.to_csv(file_path_name, index=False, sep=\"Â¤\", encoding='utf-8')\n",
    "        return df[\"TimeGenerated\"].min(), df[\"TimeGenerated\"].max()\n",
    "    \n",
    "    max_size_allowed: int = 60000000\n",
    "    need_segement, row_per_query, total_count, total_size = check_segemented_query(workspace_id, table_name, timespan, verbose=verbose, max_size_allowed=max_size_allowed)\n",
    "\n",
    "    if need_segement:\n",
    "        updated_file_path = os.path.join(file_path, table_name)\n",
    "        os.makedirs(updated_file_path, exist_ok=True)\n",
    "        \n",
    "        # get total time range in hours, conver to start and end time\n",
    "        if isinstance(timespan, timedelta):\n",
    "            timespan = (datetime.utcnow() - timespan, datetime.utcnow())\n",
    "        elif isinstance(timespan[1], timedelta):\n",
    "            total_hours = timespan[1].total_seconds() / 3600\n",
    "            timespan = (timespan[0], timespan[0] + timespan[1])\n",
    "        else:\n",
    "            total_hours = (timespan[1] - timespan[0]).total_seconds() / 3600\n",
    "        \n",
    "        # print(type(timespan[0]))\n",
    "        # get size per hour\n",
    "        size_per_hour = total_size / total_hours\n",
    "        # approximate 200MB per time chunk\n",
    "        time_chunk = int(200 * 1024 * 1024 / size_per_hour)\n",
    "        print(f\"Time chunk: {time_chunk} hours\")\n",
    "\n",
    "        tmp_start_time = timespan[0]\n",
    "        chunk_id = 0\n",
    "        # if table_name == \"AADNonInteractiveUserSignInLogs\":\n",
    "        #     # 2024-07-15 13:02:04.923788+00:00\n",
    "        #     tmp_start_time = datetime(2024, 7, 15, 13, 2, 4, 923788, tzinfo=timezone.utc) + timedelta(milliseconds=1)\n",
    "        #     chunk_id = 44\n",
    "        #     print(f\"Resuming from chunk 44, {tmp_start_time}\")\n",
    "        query_template = dedent(\"\"\"{table_name} \n",
    "| order by TimeGenerated asc\n",
    "| serialize\n",
    "| extend rn = row_number() \n",
    "| where rn <= {end}\n",
    "\"\"\")        \n",
    "        while tmp_start_time < timespan[1]:\n",
    "            # get data from a time chunk\n",
    "            tmp_timespan = (tmp_start_time, min(tmp_start_time + timedelta(hours=time_chunk), timespan[1]))\n",
    "\n",
    "            response = client.query_workspace(\n",
    "                workspace_id, \n",
    "                query_template.format(table_name=table_name, end=row_per_query),\n",
    "                timespan=tmp_timespan\n",
    "            ) # only return the first row_per_query rows\n",
    "    \n",
    "            if response.status != LogsQueryStatus.SUCCESS:\n",
    "                error = response.partial_error\n",
    "                print(f\"Getting error, retry chunk {chunk_id}\", error)\n",
    "                # update max size allowed and row per query\n",
    "                max_size_allowed -= 2000000\n",
    "                row_per_query = min(row_per_query, max_size_allowed // (total_size/total_count))\n",
    "            else:\n",
    "                # set new start time to be the last time from the response\n",
    "                earliest, latest = save_table(os.path.join(updated_file_path, f\"{table_name}_{chunk_id}.csv\"), response, need_metadata=chunk_id == 0)\n",
    "                if earliest == -1:\n",
    "                    print(f\"Reached end of the table. Exiting.\")\n",
    "                    break\n",
    "                # from pd timestamp to datetime.datetime, and +1 in milliseconds\n",
    "                tmp_start_time = latest.to_pydatetime() + timedelta(milliseconds=1)\n",
    "                chunk_id += 1\n",
    "                print(f\"Chunk {chunk_id}: {earliest} - {latest}\")  #Required span: {tmp_timespan} || Actual:\n",
    "            \n",
    "    else:\n",
    "        response = client.query_workspace(workspace_id, f\"{table_name}\", timespan=timespan)\n",
    "        save_table(os.path.join(file_path, f\"{table_name}.csv\"), response, need_metadata=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # # number of queries to run\n",
    "        # query_count = int(total_count // row_per_query) + 1\n",
    "\n",
    "        # for i in range(query_count):\n",
    "        #     start = i * row_per_query\n",
    "        #     end = min((i + 1) * row_per_query, total_count)\n",
    "        #     query = query_template.format(table_name=table_name, start=start, end=end)\n",
    "        #     response = client.query_workspace(workspace_id, query, timespan=timespan)\n",
    "\n",
    "        #     if response.status != LogsQueryStatus.SUCCESS:\n",
    "        #         error = response.partial_error\n",
    "        #         data = response.partial_data\n",
    "        #         print(error)\n",
    "        #         raise HttpResponseError(response.error)\n",
    "\n",
    "        #     save_table(os.path.join(updated_file_path, f\"{table_name}_{i}.csv\"), response, need_metadata=i==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table AADManagedIdentitySignInLogs has 24516 rows and 12.774 MB   (GB: 0.012)\n",
      "Table AADNonInteractiveUserSignInLogs has 1627812 rows and 7060.33 MB   (GB: 6.895)\n",
      "Table AADProvisioningLogs has 635 rows and 1.582 MB   (GB: 0.002)\n",
      "Table AADRiskyUsers has 175 rows and 0.034 MB   (GB: 0.0)\n",
      "Table AADServicePrincipalSignInLogs has 190271 rows and 146.289 MB   (GB: 0.143)\n",
      "Table AADUserRiskEvents has 336 rows and 0.296 MB   (GB: 0.0)\n",
      "Table Alert has 16 rows and 0.022 MB   (GB: 0.0)\n",
      "Table AmlDataLabelEvent has 1 rows and 0.0 MB   (GB: 0.0)\n",
      "Table AmlDataStoreEvent has 12 rows and 0.005 MB   (GB: 0.0)\n",
      "Table AuditLogs has 73633 rows and 137.771 MB   (GB: 0.135)\n",
      "Table AZFWApplicationRule has 222692 rows and 72.146 MB   (GB: 0.07)\n",
      "Table AZFWApplicationRuleAggregation has 49038 rows and 17.156 MB   (GB: 0.017)\n",
      "Table AZFWDnsQuery has 461803 rows and 130.201 MB   (GB: 0.127)\n",
      "Table AZFWFlowTrace has 86824 rows and 21.609 MB   (GB: 0.021)\n",
      "Table AZFWIdpsSignature has 8221 rows and 2.49 MB   (GB: 0.002)\n",
      "Table AZFWNatRule has 37717 rows and 11.69 MB   (GB: 0.011)\n",
      "Table AZFWNatRuleAggregation has 13407 rows and 4.068 MB   (GB: 0.004)\n",
      "Table AZFWNetworkRule has 56758 rows and 13.801 MB   (GB: 0.013)\n",
      "Table AZFWNetworkRuleAggregation has 2908 rows and 0.748 MB   (GB: 0.001)\n",
      "Table AZFWThreatIntel has 2200 rows and 0.611 MB   (GB: 0.001)\n",
      "Table AzureActivity has 1664 rows and 10.577 MB   (GB: 0.01)\n",
      "Table AzureMetrics has 3730824 rows and 1910.559 MB   (GB: 1.866)\n",
      "Table ContainerRegistryRepositoryEvents has 7 rows and 0.005 MB   (GB: 0.0)\n",
      "Table Heartbeat has 64787 rows and 44.322 MB   (GB: 0.043)\n",
      "Table IntuneAuditLogs has 122 rows and 0.238 MB   (GB: 0.0)\n",
      "Table IntuneDeviceComplianceOrg has 573 rows and 0.239 MB   (GB: 0.0)\n",
      "Table IntuneDevices has 573 rows and 0.349 MB   (GB: 0.0)\n",
      "Table IntuneOperationalLogs has 18 rows and 0.018 MB   (GB: 0.0)\n",
      "Table LAQueryLogs has 76828 rows and 198.798 MB   (GB: 0.194)\n",
      "Table LASummaryLogs has 36 rows and 0.009 MB   (GB: 0.0)\n",
      "Table MicrosoftAzureBastionAuditLogs has 679 rows and 0.439 MB   (GB: 0.0)\n",
      "Table MicrosoftGraphActivityLogs has 2065547 rows and 1965.194 MB   (GB: 1.919)\n",
      "Table NetworkAccessTraffic is empty.\n",
      "Table NetworkAccessTraffic has 0 rows and 0.0 MB   (GB: 0.0)\n",
      "Table Operation has 49 rows and 0.007 MB   (GB: 0.0)\n",
      "Table SigninLogs has 156454 rows and 804.006 MB   (GB: 0.785)\n",
      "Table Usage has 48949 rows and 19.314 MB   (GB: 0.019)\n",
      "Table Windows365AuditLogs has 4 rows and 0.007 MB   (GB: 0.0)\n",
      "Table AlertEvidence has 9869 rows and 14.703 MB   (GB: 0.014)\n",
      "Table AlertInfo has 951 rows and 0.214 MB   (GB: 0.0)\n",
      "Table Anomalies has 40 rows and 0.161 MB   (GB: 0.0)\n",
      "Table CloudAppEvents has 462039 rows and 817.642 MB   (GB: 0.798)\n",
      "Table DeviceEvents has 896895 rows and 980.08 MB   (GB: 0.957)\n",
      "Table DeviceFileCertificateInfo has 137227 rows and 67.341 MB   (GB: 0.066)\n",
      "Table DeviceFileEvents has 1029974 rows and 2451.239 MB   (GB: 2.394)\n",
      "Table DeviceImageLoadEvents has 143052 rows and 158.57 MB   (GB: 0.155)\n",
      "Table DeviceInfo has 20252 rows and 8.63 MB   (GB: 0.008)\n",
      "Table DeviceLogonEvents has 60276 rows and 35.267 MB   (GB: 0.034)\n",
      "Table DeviceNetworkEvents has 908779 rows and 535.267 MB   (GB: 0.523)\n",
      "Table DeviceNetworkInfo has 84619 rows and 28.522 MB   (GB: 0.028)\n",
      "Table DeviceProcessEvents has 1193749 rows and 2657.654 MB   (GB: 2.595)\n",
      "Table DeviceRegistryEvents has 940188 rows and 757.447 MB   (GB: 0.74)\n",
      "Table EmailAttachmentInfo has 15328 rows and 7.103 MB   (GB: 0.007)\n",
      "Table EmailEvents has 17727 rows and 9.943 MB   (GB: 0.01)\n",
      "Table EmailPostDeliveryEvents has 17 rows and 0.006 MB   (GB: 0.0)\n",
      "Table EmailUrlInfo has 20207 rows and 6.316 MB   (GB: 0.006)\n",
      "Table HuntingBookmark has 8 rows and 0.022 MB   (GB: 0.0)\n",
      "Table IdentityDirectoryEvents has 806 rows and 0.404 MB   (GB: 0.0)\n",
      "Table IdentityLogonEvents has 176520 rows and 89.089 MB   (GB: 0.087)\n",
      "Table IdentityQueryEvents has 31459 rows and 35.954 MB   (GB: 0.035)\n",
      "Table OfficeActivity has 39043 rows and 49.155 MB   (GB: 0.048)\n",
      "Table SecurityAlert has 14804 rows and 103.229 MB   (GB: 0.101)\n",
      "Table SecurityIncident has 1085 rows and 1.534 MB   (GB: 0.001)\n",
      "Table SentinelAudit has 21 rows and 0.102 MB   (GB: 0.0)\n",
      "Table SentinelHealth has 7934 rows and 7.569 MB   (GB: 0.007)\n",
      "Table ThreatIntelligenceIndicator has 2878437 rows and 1360.743 MB   (GB: 1.329)\n",
      "Table UrlClickEvents has 26 rows and 0.009 MB   (GB: 0.0)\n",
      "Table Watchlist has 1581 rows and 1.263 MB   (GB: 0.001)\n",
      "Total size: 22772.884 MB   (GB: 22.239)\n"
     ]
    }
   ],
   "source": [
    "ATEVET_17 = \"0fbd2874-9307-4572-b499-f8fa3cc75daf\"\n",
    "Alpine = \"e34d562e-ef12-4c4e-9bc0-7c6ae357c015\"\n",
    "# 45 days\n",
    "start_time = datetime(2024, 6, 18, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "end_time = datetime(2024, 8, 2, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# for 3 hours\n",
    "duration = timedelta(hours=3)\n",
    "# end_time = start_time + duration\n",
    "\n",
    "from download_logs import LIST_TABLES\n",
    "# # print table size of each table and total size\n",
    "total_size = 0\n",
    "for table in LIST_TABLES:\n",
    "    need_segement, row_per_query, total_count, total_size_table = check_segemented_query(Alpine, table, (start_time, end_time))\n",
    "    \n",
    "    if total_count == -1 or total_size_table == -1:\n",
    "        print(f\"Table {table} is failed to get size.\")\n",
    "        continue\n",
    "\n",
    "    total_size += total_size_table\n",
    "    print(f\"Table {table} has {total_count} rows and {round(total_size_table/1024/1024, 3)} MB   (GB: {round(total_size_table/1024/1024/1024, 3)})\")\n",
    "\n",
    "print(f\"Total size: {round(total_size/1024/1024, 3)} MB   (GB: {round(total_size/1024/1024/1024, 3)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 00:01:18.645615+00:00\n",
      "2024-06-18 00:01:18.645616+00:00\n",
      "2024-06-18 00:01:18.646615+00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Your datetime string\n",
    "datetime_str = \"2024-06-18 00:01:18.645615+00:00\"\n",
    "\n",
    "# Convert the string to a datetime object\n",
    "dt_obj = datetime.fromisoformat(datetime_str)\n",
    "\n",
    "# Print the datetime object\n",
    "print(dt_obj)\n",
    "print(dt_obj+timedelta(microseconds=1))\n",
    "print(dt_obj+timedelta(milliseconds=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table AADManagedIdentitySignInLogs is already saved.\n",
      "Table AADNonInteractiveUserSignInLogs is already saved.\n",
      "Table AADProvisioningLogs is already saved.\n",
      "Table AADRiskyUsers is already saved.\n",
      "Table AADServicePrincipalSignInLogs is already saved.\n",
      "Table AADUserRiskEvents is already saved.\n",
      "Table Alert is already saved.\n",
      "Table AmlDataLabelEvent is already saved.\n",
      "Table AmlDataStoreEvent is already saved.\n",
      "Table AuditLogs is already saved.\n",
      "Table AZFWApplicationRule is already saved.\n",
      "Table AZFWApplicationRuleAggregation is already saved.\n",
      "Table AZFWDnsQuery is already saved.\n",
      "Table AZFWFlowTrace is already saved.\n",
      "Table AZFWIdpsSignature is already saved.\n",
      "Table AZFWNatRule is already saved.\n",
      "Table AZFWNatRuleAggregation is already saved.\n",
      "Table AZFWNetworkRule is already saved.\n",
      "Table AZFWNetworkRuleAggregation is already saved.\n",
      "Table AZFWThreatIntel is already saved.\n",
      "Table AzureActivity is already saved.\n",
      "Table AzureMetrics is already saved.\n",
      "Table ContainerRegistryRepositoryEvents is already saved.\n",
      "Table Heartbeat is already saved.\n",
      "Table IntuneAuditLogs is already saved.\n",
      "Table IntuneDeviceComplianceOrg is already saved.\n",
      "Table IntuneDevices is already saved.\n",
      "Table IntuneOperationalLogs is already saved.\n",
      "Table LAQueryLogs is already saved.\n",
      "Table LASummaryLogs is already saved.\n",
      "Table MicrosoftAzureBastionAuditLogs is already saved.\n",
      "Table MicrosoftGraphActivityLogs is already saved.\n",
      "Table NetworkAccessTraffic is empty.\n",
      "Total count: 0 Size per entry: 0 Total size: 0.0 MB\n",
      "Table NetworkAccessTraffic is empty. Skipping.\n",
      "Table NetworkAccessTraffic is saved.\n",
      "Table Operation is already saved.\n",
      "Table SigninLogs is already saved.\n",
      "Table Usage is already saved.\n",
      "Table Windows365AuditLogs is already saved.\n",
      "Table AlertEvidence is already saved.\n",
      "Table AlertInfo is already saved.\n",
      "Table Anomalies is already saved.\n",
      "Table CloudAppEvents is already saved.\n",
      "Table DeviceEvents is already saved.\n",
      "Table DeviceFileCertificateInfo is already saved.\n",
      "Table DeviceFileEvents is already saved.\n",
      "Table DeviceImageLoadEvents is already saved.\n",
      "Table DeviceInfo is already saved.\n",
      "Table DeviceLogonEvents is already saved.\n",
      "Table DeviceNetworkEvents is already saved.\n",
      "Table DeviceNetworkInfo is already saved.\n",
      "Table DeviceProcessEvents is already saved.\n",
      "Table DeviceRegistryEvents is already saved.\n",
      "Table EmailAttachmentInfo is already saved.\n",
      "Table EmailEvents is already saved.\n",
      "Table EmailPostDeliveryEvents is already saved.\n",
      "Table EmailUrlInfo is already saved.\n",
      "Table HuntingBookmark is already saved.\n",
      "Table IdentityDirectoryEvents is already saved.\n",
      "Table IdentityLogonEvents is already saved.\n",
      "Table IdentityQueryEvents is already saved.\n",
      "Table OfficeActivity is already saved.\n",
      "Table SecurityAlert is already saved.\n",
      "Table SecurityIncident is already saved.\n",
      "Table SentinelAudit is already saved.\n",
      "Table SentinelHealth is already saved.\n",
      "Table: ThreatIntelligenceIndicator, Total count: 2878437, Size per entry: 479.313 Bytes, Total size: 1315.7580125627517 MB, Row per query: 125179.0, Estimate table count: 22.0\n",
      "Time chunk: 164 hours\n",
      "Getting error, retry chunk 0 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Chunk 1: 2024-06-18 00:00:00.013194+00:00 - 2024-06-19 13:30:12.511590+00:00\n",
      "Chunk 2: 2024-06-19 13:30:13.024253+00:00 - 2024-06-21 00:39:27.535038+00:00\n",
      "Chunk 3: 2024-06-21 00:39:28.861722+00:00 - 2024-06-22 15:05:51.937167+00:00\n",
      "Chunk 4: 2024-06-22 15:05:51.938287+00:00 - 2024-06-24 03:05:34.354059+00:00\n",
      "Chunk 5: 2024-06-24 03:05:35.064055+00:00 - 2024-06-25 15:05:31.046830+00:00\n",
      "Chunk 6: 2024-06-25 15:05:31.147777+00:00 - 2024-06-27 04:37:55.809938+00:00\n",
      "Chunk 7: 2024-06-27 04:37:55.931611+00:00 - 2024-06-28 23:28:43.999863+00:00\n",
      "Getting error, retry chunk 7 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Chunk 8: 2024-06-28 23:28:44.160313+00:00 - 2024-06-30 15:48:55.294829+00:00\n",
      "Chunk 9: 2024-06-30 15:48:56.155740+00:00 - 2024-07-02 09:10:21.091870+00:00\n",
      "Chunk 10: 2024-07-02 09:10:24.162031+00:00 - 2024-07-04 05:36:23.195997+00:00\n",
      "Chunk 11: 2024-07-04 05:36:23.529176+00:00 - 2024-07-06 04:18:16.996092+00:00\n",
      "Chunk 12: 2024-07-06 04:18:17.049435+00:00 - 2024-07-08 03:02:32.710951+00:00\n",
      "Chunk 13: 2024-07-08 03:02:34.344025+00:00 - 2024-07-10 10:47:54.445930+00:00\n",
      "Chunk 14: 2024-07-10 10:47:54.464195+00:00 - 2024-07-12 17:04:52.355912+00:00\n",
      "Chunk 15: 2024-07-12 17:04:53.336690+00:00 - 2024-07-14 14:31:15.338871+00:00\n",
      "Chunk 16: 2024-07-14 14:31:15.406840+00:00 - 2024-07-16 15:20:39.881362+00:00\n",
      "Chunk 17: 2024-07-16 15:20:39.882112+00:00 - 2024-07-18 15:31:06.520796+00:00\n",
      "Chunk 18: 2024-07-18 15:31:06.521134+00:00 - 2024-07-19 16:40:15.659521+00:00\n",
      "Getting error, retry chunk 18 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Chunk 19: 2024-07-19 16:40:17.020482+00:00 - 2024-07-20 07:47:09.532309+00:00\n",
      "Getting error, retry chunk 19 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Getting error, retry chunk 19 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Getting error, retry chunk 19 {'code': 'PartialError', 'message': 'There were some errors when processing your query.', 'details': [{'code': 'EngineError', 'message': 'Something went wrong processing your query on the server.', 'innererror': {'code': '-2133196797', 'message': 'The results of this query exceed the set limit of 64000000 bytes, so not all records were returned (E_QUERY_RESULT_SET_TOO_LARGE, 0x80DA0003). See https://aka.ms/kustoquerylimits for more information and possible solutions.', 'severity': 2, 'severityName': 'Error'}}], 'status': <LogsQueryStatus.FAILURE: 'Failure'>}\n",
      "Chunk 20: 2024-07-20 07:47:09.912234+00:00 - 2024-07-21 11:23:31.391461+00:00\n",
      "Chunk 21: 2024-07-21 11:23:34.161050+00:00 - 2024-07-23 09:47:18.557864+00:00\n",
      "Chunk 22: 2024-07-23 09:47:21.803643+00:00 - 2024-07-25 15:30:11.069301+00:00\n",
      "Chunk 23: 2024-07-25 15:30:11.805915+00:00 - 2024-07-27 16:31:00.813589+00:00\n",
      "Chunk 24: 2024-07-27 16:31:00.814219+00:00 - 2024-07-29 17:22:00.460678+00:00\n",
      "Chunk 25: 2024-07-29 17:22:00.474997+00:00 - 2024-08-01 06:03:17.512731+00:00\n",
      "Chunk 26: 2024-08-01 06:03:17.791634+00:00 - 2024-08-01 23:59:47.541732+00:00\n",
      "Table ThreatIntelligenceIndicator is empty. Skipping.\n",
      "Reached end of the table. Exiting.\n",
      "Table ThreatIntelligenceIndicator is saved.\n",
      "Table UrlClickEvents is already saved.\n",
      "Table Watchlist is already saved.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/alpineSkiHouse\"\n",
    "os.makedirs(file_path, exist_ok=True)\n",
    "for table in LIST_TABLES:\n",
    "    if os.path.exists(os.path.join(file_path, f\"{table}.csv\")) or os.path.exists(os.path.join(file_path, table)):\n",
    "        print(f\"Table {table} is already saved.\")\n",
    "        continue\n",
    "    try :\n",
    "        query_and_save_data(Alpine, table, (start_time, end_time), file_path, verbose=True)\n",
    "        print(f\"Table {table} is saved.\")\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"Table {table} is failed to save.\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.720703125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6882/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 24516 Size per entry: 558.68 Total size: 13.062094573974608 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, -1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segemented_query(Alpine, \"AADManagedIdentitySignInLogs\", (start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_size\n",
    "resutl = query_data(\"AADManagedIdentitySignInLogs | count\", (start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'executionTime': 0.0312273, 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 2142188, 'missbytes': 0, 'retrievebytes': 0}, 'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0}, 'bypassbytes': 0}}, 'cpu': {'user': '00:00:00.0312500', 'kernel': '00:00:00.0156250', 'totalCpu': '00:00:00.0468750', 'breakdown': {'queryExecution': '00:00:00.0468750', 'queryPlanning': '00:00:00'}}, 'memory': {'peakPerNode': 65915136}, 'network': {'interClusterTotalBytes': 2437785, 'crossClusterTotalBytes': 0}}, 'inputDatasetStatistics': {'extents': {'total': 68, 'scanned': 40, 'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z', 'scannedMaxDatetime': '2024-08-02T21:19:30.5753348Z'}, 'rows': {'total': 43242, 'scanned': 25069}, 'rowstores': {'scannedRows': 630, 'scannedValuesSize': 2124963}, 'shards': {'queriesGeneric': 33, 'queriesSpecialized': 0}}, 'datasetStatistics': [{'tableRowCount': 200, 'tableSize': 108997}], 'crossClusterResourceUsage': {}}}\n"
     ]
    }
   ],
   "source": [
    "duration = timedelta(hours=20)\n",
    "# end_time = start_time + duration\n",
    "\n",
    "resutl = query_data(\"AADManagedIdentitySignInLogs | limit 200\", (start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionTime': 0.0156376,\n",
       " 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 187666,\n",
       "     'missbytes': 0,\n",
       "     'retrievebytes': 0},\n",
       "    'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0},\n",
       "    'bypassbytes': 0}},\n",
       "  'cpu': {'user': '00:00:00.0625000',\n",
       "   'kernel': '00:00:00.0468750',\n",
       "   'totalCpu': '00:00:00.1093750',\n",
       "   'breakdown': {'queryExecution': '00:00:00.1093750',\n",
       "    'queryPlanning': '00:00:00'}},\n",
       "  'memory': {'peakPerNode': 66525440},\n",
       "  'network': {'interClusterTotalBytes': 7858560, 'crossClusterTotalBytes': 0}},\n",
       " 'inputDatasetStatistics': {'extents': {'total': 68,\n",
       "   'scanned': 40,\n",
       "   'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z',\n",
       "   'scannedMaxDatetime': '2024-08-02T21:19:30.5753348Z'},\n",
       "  'rows': {'total': 43242, 'scanned': 25069},\n",
       "  'rowstores': {'scannedRows': 627, 'scannedValuesSize': 2114383},\n",
       "  'shards': {'queriesGeneric': 4, 'queriesSpecialized': 0}},\n",
       " 'datasetStatistics': [{'tableRowCount': 1000, 'tableSize': 547144}],\n",
       " 'crossClusterResourceUsage': {}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutl.statistics['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.36287339347594"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7858560 / 547144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionTime': 0.0468879,\n",
       " 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 1771040,\n",
       "     'missbytes': 0,\n",
       "     'retrievebytes': 0},\n",
       "    'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0},\n",
       "    'bypassbytes': 0}},\n",
       "  'cpu': {'user': '00:00:00.0468750',\n",
       "   'kernel': '00:00:00.0312500',\n",
       "   'totalCpu': '00:00:00.0781250',\n",
       "   'breakdown': {'queryExecution': '00:00:00.0781250',\n",
       "    'queryPlanning': '00:00:00'}},\n",
       "  'memory': {'peakPerNode': 65546496},\n",
       "  'network': {'interClusterTotalBytes': 1267742, 'crossClusterTotalBytes': 0}},\n",
       " 'inputDatasetStatistics': {'extents': {'total': 68,\n",
       "   'scanned': 40,\n",
       "   'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z',\n",
       "   'scannedMaxDatetime': '2024-08-02T21:19:30.5753348Z'},\n",
       "  'rows': {'total': 43242, 'scanned': 25069},\n",
       "  'rowstores': {'scannedRows': 626, 'scannedValuesSize': 2110866},\n",
       "  'shards': {'queriesGeneric': 31, 'queriesSpecialized': 0}},\n",
       " 'datasetStatistics': [{'tableRowCount': 100, 'tableSize': 54988}],\n",
       " 'crossClusterResourceUsage': {}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutl.statistics['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9821961155161125"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108997/54988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionTime': 0.0312273,\n",
       " 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 2142188,\n",
       "     'missbytes': 0,\n",
       "     'retrievebytes': 0},\n",
       "    'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0},\n",
       "    'bypassbytes': 0}},\n",
       "  'cpu': {'user': '00:00:00.0312500',\n",
       "   'kernel': '00:00:00.0156250',\n",
       "   'totalCpu': '00:00:00.0468750',\n",
       "   'breakdown': {'queryExecution': '00:00:00.0468750',\n",
       "    'queryPlanning': '00:00:00'}},\n",
       "  'memory': {'peakPerNode': 65915136},\n",
       "  'network': {'interClusterTotalBytes': 2437785, 'crossClusterTotalBytes': 0}},\n",
       " 'inputDatasetStatistics': {'extents': {'total': 68,\n",
       "   'scanned': 40,\n",
       "   'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z',\n",
       "   'scannedMaxDatetime': '2024-08-02T21:19:30.5753348Z'},\n",
       "  'rows': {'total': 43242, 'scanned': 25069},\n",
       "  'rowstores': {'scannedRows': 630, 'scannedValuesSize': 2124963},\n",
       "  'shards': {'queriesGeneric': 33, 'queriesSpecialized': 0}},\n",
       " 'datasetStatistics': [{'tableRowCount': 200, 'tableSize': 108997}],\n",
       " 'crossClusterResourceUsage': {}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutl.statistics['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['resourceUsage']['network']['interClusterTotalBytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count\n",
      "0  24516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for k in resutl.tables:\n",
    "    m = pd.DataFrame(k.rows, columns=k.columns)\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12882"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutl.statistics['query']['resourceUsage']['network']['interClusterTotalBytes']\n",
    "# bytes to mib\n",
    "# resutl.statistics['query']['resourceUsage']['network']['interClusterTotalBytes'] / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'executionTime': 0.0156209, 'resourceUsage': {'cache': {'shards': {'hot': {'hitbytes': 35587, 'missbytes': 0, 'retrievebytes': 0}, 'cold': {'hitbytes': 0, 'missbytes': 0, 'retrievebytes': 0}, 'bypassbytes': 0}}, 'cpu': {'user': '00:00:00', 'kernel': '00:00:00', 'totalCpu': '00:00:00', 'breakdown': {'queryExecution': '00:00:00', 'queryPlanning': '00:00:00'}}, 'memory': {'peakPerNode': 65546752}, 'network': {'interClusterTotalBytes': 12882, 'crossClusterTotalBytes': 0}}, 'inputDatasetStatistics': {'extents': {'total': 68, 'scanned': 1, 'scannedMinDatetime': '2024-06-18T00:00:00.0000000Z', 'scannedMaxDatetime': '2024-06-18T00:00:00.0000000Z'}, 'rows': {'total': 43242, 'scanned': 215}, 'rowstores': {'scannedRows': 611, 'scannedValuesSize': 2061095}, 'shards': {'queriesGeneric': 1, 'queriesSpecialized': 0}}, 'datasetStatistics': [{'tableRowCount': 1, 'tableSize': 603}], 'crossClusterResourceUsage': {}}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m duration \u001b[38;5;241m=\u001b[39m timedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# end_time = start_time + duration\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m resutl \u001b[38;5;241m=\u001b[39m \u001b[43mquery_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAADManagedIdentitySignInLogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(resutl)\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(query, timpespan)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mstatistics)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# get total bytes processed\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatistics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnetwork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterClusterTotalBytes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m LogsQueryStatus\u001b[38;5;241m.\u001b[39mSUCCESS:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtables\n",
      "\u001b[1;31mKeyError\u001b[0m: 'network'"
     ]
    }
   ],
   "source": [
    "\n",
    "ATEVET_17 = \"0fbd2874-9307-4572-b499-f8fa3cc75daf\"\n",
    "Alpine = \"e34d562e-ef12-4c4e-9bc0-7c6ae357c015\"\n",
    "def query_data(query, timpespan: Optional[Union[timedelta, Tuple[datetime, timedelta], Tuple[datetime, datetime]]]):\n",
    "    response = client.query_workspace(Alpine, query, timespan=timpespan, include_statistics=True)\n",
    "    print(response.statistics)\n",
    "    # get total bytes processed\n",
    "    print(response.statistics[\"network\"]['interClusterTotalBytes'])\n",
    "    if response.status == LogsQueryStatus.SUCCESS:\n",
    "        return response.tables\n",
    "    else:\n",
    "        return response.partial_data\n",
    "    \n",
    "    \n",
    "# 45 days\n",
    "start_time = datetime(2024, 6, 18, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "end_time = datetime(2024, 8, 2, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# for 3 hours\n",
    "duration = timedelta(hours=3)\n",
    "# end_time = start_time + duration\n",
    "\n",
    "resutl = query_data(\"AADManagedIdentitySignInLogs\", (start_time, duration))\n",
    "print(resutl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogsTable' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get stats\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mresutl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogsTable' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "# get stats\n",
    "stats = resutl[0].to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
