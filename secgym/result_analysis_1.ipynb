{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def analysis(data:dict, verbose:bool=False):\n",
    "    \"\"\"Analyze the data and print out the results\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data to be analyzed, load an \"agent log\" json file\n",
    "        \n",
    "    \"\"\"\n",
    "    total_len = len(data)\n",
    "    total_reward = 0\n",
    "    total_round = 0\n",
    "    non_zero_reward_count = 0\n",
    "    non_success_count = 0\n",
    "\n",
    "    path_count = {}\n",
    "    reward_count = {}\n",
    "    round_count = {}\n",
    "    for k in data:\n",
    "        p = len(k['question_dict']['shortest_alert_path'])\n",
    "        if p not in path_count:\n",
    "            path_count[p] = 0\n",
    "            reward_count[p] = 0\n",
    "            round_count[p] = 0\n",
    "\n",
    "        if k['reward'] > 0 and k['reward'] < 1:\n",
    "            non_success_count += 1\n",
    "        if k['reward'] > 0:\n",
    "            non_zero_reward_count += 1\n",
    "        \n",
    "        # total\n",
    "        total_reward += k['reward']\n",
    "        total_round += (len(k[\"messages\"]) - 1) // 2\n",
    "\n",
    "        path_count[p] += 1\n",
    "        reward_count[p] += k['reward']\n",
    "        round_count[p] += (len(k[\"messages\"]) - 1) // 2\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Average reward: {total_reward}/{total_len} = {round(total_reward/total_len,6)}\")\n",
    "        print(f\"Average round: {total_round}/{total_len} = {round(total_round/total_len,6)}\")\n",
    "        # print(f\"Non success / non-zero reward count: {non_success_count}/{non_zero_reward_count}\")\n",
    "\n",
    "        # sorted_keys = sorted(path_count.keys())\n",
    "        # for k in sorted_keys:\n",
    "        #     print(f\"Difficulty {k}: {round(reward_count[k], 2)}/{path_count[k]} = {round(reward_count[k]/path_count[k],6)} | Avg round: {round(round_count[k]/path_count[k], 2)}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"total_len\": total_len,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"total_round\": total_round,\n",
    "        \"non_zero_reward_count\": non_zero_reward_count,\n",
    "        \"non_success_count\": non_success_count,\n",
    "        \"path_count\": path_count,\n",
    "        \"reward_count\": reward_count,\n",
    "        \"round_count\": round_count\n",
    "    }\n",
    "\n",
    "import json\n",
    "\n",
    "def analysis_one_run(log_path, file_template, print_total=False, sample_size=-1):\n",
    "\n",
    "    incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "\n",
    "    total_len = 0\n",
    "    total_reward = 0\n",
    "    total_round = 0\n",
    "    path_count = {}\n",
    "    reward_count = {}\n",
    "\n",
    "    all_data = []\n",
    "    for i in incidents:\n",
    "        if not print_total:\n",
    "            print(\"*\"*20)\n",
    "            print(f\"Analysis for incident {i}\")\n",
    "\n",
    "        a = open(f\"{log_path}/{file_template.format(i)}\", \"r\")\n",
    "        b = json.load(a)\n",
    "        all_data.extend(b)\n",
    "    print(len(all_data))\n",
    "        \n",
    "    random.shuffle(all_data)\n",
    "    if sample_size > 0:\n",
    "        # sample_size is the size for each difficulty, For difficulty 1 3 5 7 9, we will sample sample_size number of incidents\n",
    "        # go through the data and sample sample_size number for each difficulty\n",
    "        # go through each problem, if the difficulty reaches sample_size, won't append it\n",
    "        sampled_data = []\n",
    "        sampled_count = {}\n",
    "        for k in all_data:\n",
    "            p = len(k['question_dict']['shortest_alert_path'])\n",
    "            if p not in sampled_count:\n",
    "                sampled_count[p] = 0\n",
    "            if sampled_count[p] < sample_size:\n",
    "                sampled_data.append(k)\n",
    "                sampled_count[p] += 1\n",
    "        all_data = sampled_data\n",
    "        \n",
    "    result_dict = analysis(all_data, not print_total)\n",
    "\n",
    "    total_len += result_dict['total_len']\n",
    "    total_reward += result_dict['total_reward']\n",
    "    total_round += result_dict['total_round']\n",
    "    for k in result_dict['path_count']:\n",
    "        if k not in path_count:\n",
    "            path_count[k] = 0\n",
    "            reward_count[k] = 0\n",
    "\n",
    "        path_count[k] += result_dict['path_count'][k]\n",
    "        reward_count[k] += result_dict['reward_count'][k]\n",
    "    if not print_total:\n",
    "        print(\"*\"*20)\n",
    "        \n",
    "    if print_total:\n",
    "        print(\"*\"*40)\n",
    "        print(\"*\"*40)\n",
    "        print(\"Total analysis\")\n",
    "        print(f\"Total length: {total_len}\")\n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "        print(f\"Total round: {total_round}\")\n",
    "        print(f\"Average reward: {total_reward}/{total_len} = {round(total_reward/total_len,6)}\")\n",
    "        print(f\"Average round: {total_round}/{total_len} = {round(total_round/total_len,6)}\")\n",
    "\n",
    "        sorted_keys = sorted(path_count.keys())\n",
    "        for k in sorted_keys:\n",
    "            print(f\"Difficulty {k}: {round(reward_count[k], 2)}/{path_count[k]} = {round(reward_count[k]/path_count[k],6)}\")\n",
    "\n",
    "\n",
    "def get_correct_problem_ids(log_path, file_template, incident_id):\n",
    "    a = open(f\"{log_path}/{file_template.format(incident_id)}\", \"r\")\n",
    "    b = json.load(a)\n",
    "    correct_ids = []\n",
    "    for k in b:\n",
    "        if k['reward'] == 1:\n",
    "            correct_ids.append(k['question_id'])\n",
    "    return correct_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "log_path = \"C:/Users/amudgerikar/source/repos/SecRL/secgym/results/\"\n",
    "file_template = \"PromptSauceAgent_incident_166_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "#incidents = [5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 207.32000000000005\n",
      "Total round: 7013\n",
      "Average reward: 207.32000000000005/676 = 0.306686\n",
      "Average round: 7013/676 = 10.37426\n",
      "Difficulty 1: 27.16/66 = 0.411515\n",
      "Difficulty 3: 115.6/402 = 0.287562\n",
      "Difficulty 5: 42.16/127 = 0.331969\n",
      "Difficulty 7: 17.4/71 = 0.24507\n",
      "Difficulty 9: 5/10 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/base_agent_experiments_4o/alert_level\"\n",
    "file_template = \"incident_{}_agent_log_gpt-4o_46_alert.json\"\n",
    "\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method\n",
    "\n",
    "baseline | 30.7\n",
    "refined_prompt | 40.0\n",
    "refined_prompt (step=50) | 49.2\n",
    "reflexion | 51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 344.864\n",
      "Total round: 5846\n",
      "Average reward: 344.864/676 = 0.510154\n",
      "Average round: 5846/676 = 8.647929\n",
      "Difficulty 1: 39/66 = 0.590909\n",
      "Difficulty 3: 221.4/402 = 0.550746\n",
      "Difficulty 5: 57/127 = 0.448819\n",
      "Difficulty 7: 26.46/71 = 0.372732\n",
      "Difficulty 9: 1/10 = 0.1\n"
     ]
    }
   ],
   "source": [
    "# reflexion + GPT-4o\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/reflexion_agent_experiments_4o/alert_level/steps=15\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Sauce Agent GPT-4o\n",
      "Step = 15\n",
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 270.18399999999997\n",
      "Total round: 6978\n",
      "Average reward: 270.18399999999997/676 = 0.39968\n",
      "Average round: 6978/676 = 10.322485\n",
      "Difficulty 1: 26/66 = 0.393939\n",
      "Difficulty 3: 161.6/402 = 0.40199\n",
      "Difficulty 5: 51.12/127 = 0.40252\n",
      "Difficulty 7: 28.46/71 = 0.400901\n",
      "Difficulty 9: 3/10 = 0.3\n",
      "Step = 50\n",
      "654\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 654\n",
      "Total reward: 321.66399999999993\n",
      "Total round: 10638\n",
      "Average reward: 321.66399999999993/654 = 0.491841\n",
      "Average round: 10638/654 = 16.266055\n",
      "Difficulty 1: 33.72/66 = 0.510909\n",
      "Difficulty 3: 196.36/380 = 0.516737\n",
      "Difficulty 5: 51.72/127 = 0.407244\n",
      "Difficulty 7: 34.86/71 = 0.491042\n",
      "Difficulty 9: 5/10 = 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt Sauce Agent GPT-4o\")\n",
    "\n",
    "print(\"Step = 15\")\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/prompt_sauce_agent_experiments_4o/alert_level/steps=15\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_46_alert_sum.json\"\n",
    "analysis_one_run(path, file_template, True)\n",
    "\n",
    "\n",
    "print(\"Step = 50\")\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/prompt_sauce_agent_experiments_4o/alert_level/steps=50\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_46_alert_sum.json\"\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap of correct questions with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_correct_problem_ids(log_path, file_template, incident_id):\n",
    "    with open(f\"{log_path}/{file_template.format(incident_id)}\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    correct_ids = []\n",
    "    for i, entry in enumerate(data):\n",
    "        if entry['reward'] == 1:\n",
    "            correct_ids.append(i)\n",
    "    return correct_ids\n",
    "\n",
    "def jaccard_similarity(path_1, template_1, path_2, template_2, incidents):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard similarity between two agents across multiple incidents.\n",
    "\n",
    "    Args:\n",
    "        path_1 (str): Path to logs for Agent 1.\n",
    "        template_1 (str): File template for Agent 1's logs.\n",
    "        path_2 (str): Path to logs for Agent 2.\n",
    "        template_2 (str): File template for Agent 2's logs.\n",
    "        incidents (list of int): List of incident IDs to compare.\n",
    "\n",
    "    Returns:\n",
    "        float: The overall Jaccard similarity across all incidents.\n",
    "    \"\"\"\n",
    "    union_set = set()\n",
    "    intersection_set = set()\n",
    "\n",
    "    offset = 0  # Initialize offset to ensure unique IDs across incidents\n",
    "\n",
    "    for incident_id in incidents:\n",
    "        # Retrieve correct question IDs for each agent and apply offset\n",
    "        correct_1 = {qid + offset for qid in get_correct_problem_ids(path_1, template_1, incident_id)}\n",
    "        correct_2 = {qid + offset for qid in get_correct_problem_ids(path_2, template_2, incident_id)}\n",
    "\n",
    "        print(f\"Incident {incident_id}\")\n",
    "        print(f\"Correct 1: {correct_1}\")\n",
    "        print(f\"Correct 2: {correct_2}\")\n",
    "        print()\n",
    "        \n",
    "        # Update union and intersection sets\n",
    "        union_set.update(correct_1 | correct_2)\n",
    "        intersection_set.update(correct_1 & correct_2)\n",
    "        \n",
    "        # Increment offset to avoid ID conflicts for the next incident\n",
    "        offset += max(len(correct_1), len(correct_2)) + 1\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    if not union_set:  # To handle cases with no data\n",
    "        return 0.0\n",
    "    return len(intersection_set) / len(union_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident 55\n",
      "Correct 1: {0, 4, 8, 14, 21, 24, 26, 29, 30, 31, 32, 38, 39, 41, 46, 47, 51, 54, 57, 58, 59, 61, 70, 78, 79, 86, 93, 94, 96, 98, 99}\n",
      "Correct 2: {0, 4, 7, 8, 11, 13, 14, 22, 23, 24, 28, 30, 31, 32, 34, 35, 38, 43, 44, 47, 48, 51, 53, 56, 57, 62, 67, 69, 75, 76, 78, 81, 83, 98}\n",
      "\n",
      "Incident 5\n",
      "Correct 1: {133, 134, 36, 37, 38, 39, 52, 54, 58, 60, 62, 66, 67, 68, 69, 73, 74, 76, 87, 94, 95, 97, 99, 100, 103, 112, 114, 117, 119, 123, 124, 125}\n",
      "Correct 2: {130, 131, 133, 35, 36, 37, 38, 39, 40, 43, 45, 47, 52, 53, 54, 56, 62, 65, 67, 68, 73, 75, 89, 90, 91, 92, 93, 94, 95, 98, 99, 103, 110, 111, 112, 113, 114, 118, 119, 124, 125}\n",
      "\n",
      "Incident 34\n",
      "Correct 1: {129, 131, 132, 133, 143, 163, 166, 169, 79, 80, 81, 82, 84, 91, 92, 94, 103, 104, 112, 114, 116, 125}\n",
      "Correct 2: {129, 130, 131, 132, 133, 135, 136, 139, 140, 143, 144, 146, 147, 148, 152, 153, 154, 160, 163, 164, 165, 166, 168, 169, 170, 172, 175, 176, 77, 78, 79, 81, 84, 91, 92, 93, 94, 96, 97, 98, 105, 106, 108, 110, 111, 113, 114, 115, 116, 117, 120, 121, 122, 125, 126, 127}\n",
      "\n",
      "Incident 38\n",
      "Correct 1: {146, 140, 141}\n",
      "Correct 2: {135, 138, 139, 140, 143, 144, 146, 147, 149}\n",
      "\n",
      "Incident 134\n",
      "Correct 1: {149, 150, 151, 152, 154, 159, 162, 166, 171, 174, 175, 176, 185, 187, 188, 189, 194, 195, 196, 199, 201, 205, 206, 218, 219, 224}\n",
      "Correct 2: {144, 145, 147, 148, 150, 152, 154, 155, 156, 158, 159, 160, 162, 164, 166, 167, 168, 171, 172, 173, 174, 175, 178, 179, 180, 185, 186, 188, 189, 190, 192, 194, 195, 196, 197, 198, 200, 201, 205, 207, 208, 209, 211, 216, 219, 220, 224}\n",
      "\n",
      "Incident 166\n",
      "Correct 1: {257, 261, 263, 268, 273, 274, 277, 280, 281, 284, 285, 289, 194, 195, 196, 197, 199, 203, 206, 207, 218, 219, 222, 223, 227, 233, 246, 254}\n",
      "Correct 2: {256, 257, 261, 263, 265, 268, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 193, 194, 195, 197, 199, 203, 204, 205, 206, 207, 210, 211, 213, 215, 218, 219, 220, 222, 223, 227, 233, 236, 238, 239, 242, 246, 252, 253, 254}\n",
      "\n",
      "Incident 39\n",
      "Correct 1: {256, 257, 258, 260, 261, 263, 264, 265, 274, 277, 278, 283, 285, 286, 287, 291, 292, 295, 300, 308, 310, 319, 321, 322, 327, 328, 338, 245, 250, 253}\n",
      "Correct 2: {256, 257, 258, 260, 261, 262, 264, 265, 267, 268, 269, 272, 274, 275, 277, 278, 279, 280, 283, 284, 286, 287, 288, 290, 292, 294, 295, 300, 301, 304, 308, 309, 310, 311, 319, 321, 322, 324, 326, 328, 333, 338, 340, 248, 249, 250, 252, 253, 254}\n",
      "\n",
      "Incident 322\n",
      "Correct 1: {296, 297, 299, 301, 302, 304, 305, 308, 309, 310, 312, 313, 314, 318, 319, 320, 321, 327, 329, 331, 337, 340, 341, 343, 346, 348, 349, 354, 357, 363, 371}\n",
      "Correct 2: {295, 296, 297, 299, 301, 302, 303, 304, 305, 308, 310, 311, 312, 313, 314, 315, 318, 319, 320, 321, 322, 323, 327, 328, 329, 330, 331, 333, 334, 337, 338, 339, 340, 341, 343, 346, 348, 349, 350, 351, 354, 355, 357, 358, 359, 363, 364, 365, 366, 367, 368, 371, 372, 373}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4755244755244755"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "path_1 = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/base_agent_experiments_4o/alert_level\"\n",
    "template_1 = \"incident_{}_agent_log_gpt-4o_46_alert.json\"\n",
    "\n",
    "# Reflexion\n",
    "path_2 = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/reflexion_agent_experiments_4o/alert_level/steps=15\"\n",
    "template_2 = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "\n",
    "jaccard_similarity(path_1=path_1, template_1=template_1, path_2=path_2, template_2=template_2, incidents=incidents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### GPT-4o-mini\n",
    "\n",
    "| Incident Number  | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|--------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| Alert Counted (Sorted by) | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "| Log              | 0.081   | 0.137   | 0.099  | 0.278   | 0.254   | 0.215   | 0.318    | 0.025   |\n",
    "| Alert            | 0.111   | 0.218   | 0.150  | 0.264   | 0.236   | 0.252   | 0.316    | 0.088   |\n",
    "| Incident         | 0.094   | 0.144   | 0.214  | 0.318   | 0.206   | 0.254   | 0.253    | 0.063   |\n",
    "\n",
    "\n",
    "<!-- \n",
    "### GPT-4o Log Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "| **Average Reward**  | 0.070   | 0.186   | 0.172  | 0.194   | 0.190   | 0.257   | 0.385    | 0.125   |\n",
    "| **Average Round**   | 12.08   | 10.88   | 11.6   | 11.52   | 11.13   | 10.667  | 9.987    | 11.375  |\n",
    "\n",
    "    total_len += result_dict['total_len']\n",
    "    total_reward += result_dict['total_reward']\n",
    "    total_round += result_dict['total_round']\n",
    "    \n",
    "    for k in result_dict['path_count']:\n",
    "        if k not in path_count:\n",
    "            path_count[k] = 0\n",
    "            reward_count[k] = 0\n",
    "\n",
    "### GPT-4o Incident Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.234   | 0.287   | 0.262  | 0.218   | 0.314   | 0.331   | 0.435    | 0.188   |\n",
    "| **Average Round**   | 10.05   | 9.98    | 10.27  | 10.87   | 10.39   | 10.370  | 9.684    | 11.688  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "\n",
    "### GPT-4o-mini Log Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.081   | 0.137   | 0.099  | 0.278   | 0.254   | 0.215   | 0.318    | 0.025   |\n",
    "| **Average Round**   | 12.24   | 12.28   | 11.7   | 10.83   | 10.85   | 11.469  | 11.405   | 11.438  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "### GPT-4o-mini Alert Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.111   | 0.218   | 0.150  | 0.264   | 0.236   | 0.252   | 0.316    | 0.088   |\n",
    "| **Average Round**   | 11.41   | 11.92   | 11.84  | 10.82   | 10.9    | 11.123  | 10.937   | 11.188  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "### GPT-4o-mini Incident Level\n",
    "\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.094   | 0.144   | 0.214  | 0.318   | 0.206   | 0.254   | 0.253    | 0.063   |\n",
    "| **Average Round**   | 11.58   | 11.92   | 11.54  | 10.84   | 10.89   | 11.309  | 11.025   | 10.313  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       | -->\n",
    "\n",
    "# Difficulty Reward Change Table\n",
    "\n",
    "<!-- | **Model**    | **Level**       | **Reward Change (1 → 3 → 5 → 7 → 9)**      | **Avg Reward** | **Avg Round** |\n",
    "|--------------|-----------------|---------------------------------------------|---------------|---------------|\n",
    "| GPT-4o       | Log Level       | 0.326667 → 0.192438 → 0.181102 → 0.132169 → 0.3 | 0.198675      | 11.177515     |\n",
    "| GPT-4o-mini  | Log Level       | 0.35697 → 0.201493 → 0.143307 → 0.071324 → 0.0  | 0.189089      | 11.542899     |\n",
    "| GPT-4o       | Alert Level     | 0.411515 → 0.287562 → 0.331969 → 0.24507 → 0.5  | 0.306686      | 10.37426      |\n",
    "| GPT-4o-mini  | Alert Level     | 0.341818 → 0.210945 → 0.205669 → 0.155831 → 0.0 | 0.213822      | 11.29142      |\n",
    "| GPT-4o       | Incident Level  | 0.366061 → 0.273632 → 0.308346 → 0.258028 → 0.4 | 0.289408      | 10.278107     |\n",
    "| GPT-4o-mini  | Incident Level  | 0.320606 → 0.20995 → 0.154016 → 0.169014 → 0.2  | 0.205799      | 11.285503     | -->\n",
    "\n",
    "| **Model**    | **Level**       | **Reward Change (1 → 3 → 5 → 7 → 9)**      | **Avg Reward** | **Avg Round** |\n",
    "|--------------|-----------------|---------------------------------------------|---------------|---------------|\n",
    "| GPT-4o       | Log Level       | 0.327 → 0.192 → 0.181 → 0.132 → 0.300      | 0.199         | 11.178        |\n",
    "| GPT-4o-mini  | Log Level       | 0.357 → 0.201 → 0.143 → 0.071 → 0.000      | 0.189         | 11.543        |\n",
    "| GPT-4o       | Alert Level     | 0.412 → 0.288 → 0.332 → 0.245 → 0.500      | 0.307         | 10.374        |\n",
    "| GPT-4o-mini  | Alert Level     | 0.342 → 0.211 → 0.206 → 0.156 → 0.000      | 0.214         | 11.291        |\n",
    "| GPT-4o       | Incident Level  | 0.366 → 0.274 → 0.308 → 0.258 → 0.400      | 0.289         | 10.278        |\n",
    "| GPT-4o-mini  | Incident Level  | 0.321 → 0.210 → 0.154 → 0.169 → 0.200      | 0.206         | 11.286        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Validation test\n",
    "\n",
    "1. With higher difficulty, the reward should be lower.\n",
    "2. More advanced model should have higher reward.\n",
    "3. Are the correct questions always the same?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
