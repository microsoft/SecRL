{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "import json\n",
    "\n",
    "def analysis(data:dict, verbose:bool=False):\n",
    "    \"\"\"Analyze the data and print out the results\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data to be analyzed, load an \"agent log\" json file\n",
    "        \n",
    "    \"\"\"\n",
    "    total_len = len(data)\n",
    "    total_reward = 0\n",
    "    total_round = 0\n",
    "    \n",
    "    success_count = 0 # reward == 1\n",
    "    non_zero_reward_count = 0 # reward > 0\n",
    "    submit_count = 0\n",
    "\n",
    "    path_count = {}\n",
    "    reward_count = {} # reward for each difficulty\n",
    "    round_count = {}\n",
    "    eval_error_count = 0\n",
    "\n",
    "    # cost\n",
    "    total_cost = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "\n",
    "    # query efficiency\n",
    "    empty_result_count = 0\n",
    "    error_query_count = 0\n",
    "    query_count = 0\n",
    "\n",
    "    success_non_empty_query_count = 0\n",
    "    potential_good_example_count = 0\n",
    "\n",
    "    fail_to_run_count = 0\n",
    "    max_round = 0\n",
    "    min_round = 1000\n",
    "\n",
    "\n",
    "    for ii, k in enumerate(data):\n",
    "        if k.get(\"usage_summary\") is None:\n",
    "            print(\"no usage summary\", ii)\n",
    "            fail_to_run_count += 1\n",
    "            continue\n",
    "\n",
    "        p = len(k['question_dict']['shortest_alert_path'])\n",
    "        if p not in path_count:\n",
    "            path_count[p] = 0\n",
    "            reward_count[p] = 0\n",
    "            round_count[p] = 0\n",
    "\n",
    "        if k['reward'] > 0:\n",
    "            non_zero_reward_count += 1\n",
    "        if k['reward'] == 1:\n",
    "            success_count += 1\n",
    "        \n",
    "        # total\n",
    "        total_reward += k['reward']\n",
    "        total_round += (len(k[\"messages\"]) - 1) // 2\n",
    "\n",
    "        path_count[p] += 1\n",
    "        reward_count[p] += k['reward']\n",
    "        round_count[p] += (len(k[\"messages\"]) - 1) // 2\n",
    "        max_round = max(max_round, (len(k[\"messages\"]) - 1) // 2)\n",
    "        min_round = min(min_round, (len(k[\"messages\"]) - 1) // 2)\n",
    "        submit_count += k['info'].get(\"submit\", 0)\n",
    "\n",
    "        if \"error\" in k['info'] or k['info'].get(\"is_json_success\") == False or k['info'].get(\"is_reflect_success\") == False:\n",
    "            eval_error_count += 1\n",
    "\n",
    "        try:\n",
    "            total_cost += k['usage_summary']['total_cost']\n",
    "        except Exception as e:\n",
    "            print(k)\n",
    "            raise e\n",
    "\n",
    "        model = list(k['usage_summary'].keys())[-1]\n",
    "        total_prompt_tokens += k['usage_summary'][model]['prompt_tokens']\n",
    "        total_completion_tokens += k['usage_summary'][model]['completion_tokens']\n",
    "\n",
    "        success_select_count_per_problem = 0\n",
    "        for i, m in enumerate(k['messages']):\n",
    "            content = m['content']\n",
    "            if m['role'] == \"user\":\n",
    "                if \"ProgrammingError\" in content or \"DataError\" in content:\n",
    "                    error_query_count += 1\n",
    "                elif content == \"[]\" or content == \"\":\n",
    "                    empty_result_count += 1\n",
    "                elif i > 0 and k['messages'][i-1]['role'] == \"assistant\" and  \"execute[SELECT\" in k['messages'][i-1]['content']: \n",
    "                    success_select_count_per_problem += 1\n",
    "            if m['role'] == \"assistant\" and \"Action: execute[\" in content:\n",
    "                query_count += 1\n",
    "            \n",
    "        if success_select_count_per_problem >= 4:\n",
    "            potential_good_example_count += 1\n",
    "    print(f\"Max round: {max_round}, Min round: {min_round}\")\n",
    "                \n",
    "    if verbose:\n",
    "        print(f\"Average reward: {total_reward}/{total_len} = {round(total_reward/total_len,6)}\")\n",
    "        print(f\"Average round: {total_round}/{total_len} = {round(total_round/total_len,6)}\")\n",
    "\n",
    "        # sorted_keys = sorted(path_count.keys())\n",
    "        # for k in sorted_keys:\n",
    "        #     print(f\"Difficulty {k}: {round(reward_count[k], 2)}/{path_count[k]} = {round(reward_count[k]/path_count[k],6)} | Avg round: {round(round_count[k]/path_count[k], 2)}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"total_len\": total_len,\n",
    "        \"potential_good_example_count\": potential_good_example_count,\n",
    "\n",
    "        # Performance Analysis\n",
    "        \"total_reward\": total_reward,\n",
    "        \"success_count\": success_count, # reward==1\n",
    "        \"non_zero_reward_count\": non_zero_reward_count, # for evaluator usefullness\n",
    "        \"submit_count\": submit_count,\n",
    "\n",
    "        # Efficiency / Computational Analysis\n",
    "        \"total_cost\": total_cost,\n",
    "        \"total_prompt_tokens\": total_prompt_tokens,\n",
    "        \"total_completion_tokens\": total_completion_tokens,\n",
    "        \"total_round\": total_round,\n",
    "\n",
    "        # Query Efficiency\n",
    "        \"empty_result_count\": empty_result_count,\n",
    "        \"error_query_count\": error_query_count,\n",
    "        \"query_count\": query_count,\n",
    "\n",
    "        # For difficulty\n",
    "        \"path_count\": path_count,\n",
    "        \"round_count\": round_count,\n",
    "        \"reward_count\": reward_count,\n",
    "\n",
    "        # Evaluation Analysis\n",
    "        \"eval_error_count\": eval_error_count,\n",
    "\n",
    "        \"fail_to_run_count\": fail_to_run_count\n",
    "    }\n",
    "\n",
    "\n",
    "def print_analysis(result_dict:dict, head:str=None):\n",
    "    print(\"*\"*40)\n",
    "    if head:\n",
    "        print(f\"{head} analysis\")\n",
    "    problem_count = result_dict['total_len']\n",
    "\n",
    "    print(f\"Average reward: {result_dict['total_reward']}/{problem_count} = {round(result_dict['total_reward']/problem_count,4)}\")\n",
    "    print(f\"Success rate: {result_dict['success_count']}/{problem_count} = {round(result_dict['success_count']/problem_count * 100,2)}%\")\n",
    "    \n",
    "    # Computational Analysis\n",
    "    print(f\"** Computational Analysis:\")\n",
    "    print(f\"Average round: {result_dict['total_round']}/{problem_count} = {round(result_dict['total_round']/problem_count,4)}\")\n",
    "    print(f\"Average cost: {result_dict['total_cost']}/{problem_count} = {round(result_dict['total_cost']/problem_count,4)}\")\n",
    "    print(f\"Average prompt tokens: {result_dict['total_prompt_tokens']}/{problem_count} = {round(result_dict['total_prompt_tokens']/problem_count,4)}\")\n",
    "    print(f\"Average completion tokens: {result_dict['total_completion_tokens']}/{problem_count} = {round(result_dict['total_completion_tokens']/problem_count,4)}\")\n",
    "\n",
    "    print(\"** Query Efficiency: \")\n",
    "    success_query_count = result_dict['query_count'] - result_dict['error_query_count']\n",
    "    print(f'Success Query rate: {success_query_count}/{result_dict[\"query_count\"]} = {round(success_query_count/result_dict[\"query_count\"] * 100,2)}%')\n",
    "    print(f'Success Non-Empty Query rate: {success_query_count-result_dict[\"empty_result_count\"]}/{result_dict[\"query_count\"]} = {round((success_query_count-result_dict[\"empty_result_count\"])/result_dict[\"query_count\"] * 100,2)}%')\n",
    "    \n",
    "    print(f\"Potential Good Example Count: {result_dict['potential_good_example_count']} / {problem_count}\")\n",
    "    print(f\"Fail to run count: {result_dict['fail_to_run_count']}\")\n",
    "\n",
    "def analysis_one_run(log_path, file_template, print_total=False, sample_size=-1):\n",
    "\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "\n",
    "    total_len = 0\n",
    "    total_reward = 0\n",
    "    total_round = 0\n",
    "    path_count = {}\n",
    "    reward_count = {}\n",
    "\n",
    "    all_data = []\n",
    "    for i in incidents:\n",
    "        if not print_total:\n",
    "            print(\"*\"*20)\n",
    "            print(f\"Analysis for incident {i}\")\n",
    "\n",
    "        a = open(f\"{log_path}/{file_template.format(i)}\", \"r\")\n",
    "        b = json.load(a)\n",
    "        all_data.extend(b)\n",
    "    print(len(all_data))\n",
    "        \n",
    "    random.shuffle(all_data)\n",
    "    if sample_size > 0:\n",
    "        # sample_size is the size for each difficulty, For difficulty 1 3 5 7 9, we will sample sample_size number of incidents\n",
    "        # go through the data and sample sample_size number for each difficulty\n",
    "        # go through each problem, if the difficulty reaches sample_size, won't append it\n",
    "        sampled_data = []\n",
    "        sampled_count = {}\n",
    "        for k in all_data:\n",
    "            p = len(k['question_dict']['shortest_alert_path'])\n",
    "            if p not in sampled_count:\n",
    "                sampled_count[p] = 0\n",
    "            if sampled_count[p] < sample_size:\n",
    "                sampled_data.append(k)\n",
    "                sampled_count[p] += 1\n",
    "        all_data = sampled_data\n",
    "        \n",
    "    result_dict = analysis(all_data, not print_total)\n",
    "\n",
    "    total_len += result_dict['total_len']\n",
    "    total_reward += result_dict['total_reward']\n",
    "    total_round += result_dict['total_round']\n",
    "    for k in result_dict['path_count']:\n",
    "        if k not in path_count:\n",
    "            path_count[k] = 0\n",
    "            reward_count[k] = 0\n",
    "\n",
    "        path_count[k] += result_dict['path_count'][k]\n",
    "        reward_count[k] += result_dict['reward_count'][k]\n",
    "    if not print_total:\n",
    "        print(\"*\"*20)\n",
    "        \n",
    "    if print_total:\n",
    "        print(\"*\"*40)\n",
    "        print(\"*\"*40)\n",
    "        print(\"Total analysis\")\n",
    "        print(f\"Total length: {total_len}\")\n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "        print(f\"Total round: {total_round}\")\n",
    "        print(f\"Average reward: {total_reward}/{total_len} = {round(total_reward/total_len,6)}\")\n",
    "        print(f\"Average success rate: {result_dict['success_count']}/{total_len} = {round(result_dict['success_count']/total_len,6)}\")\n",
    "        print(f\"Average round: {total_round}/{total_len} = {round(total_round/total_len,6)}\")\n",
    "\n",
    "        sorted_keys = sorted(path_count.keys())\n",
    "        for k in sorted_keys:\n",
    "            print(f\"Difficulty {k}: {round(reward_count[k], 2)}/{path_count[k]} = {round(reward_count[k]/path_count[k],6)}\")\n",
    "\n",
    "\n",
    "def get_correct_problem_ids(log_path, file_template, incident_id):\n",
    "    a = open(f\"{log_path}/{file_template.format(incident_id)}\", \"r\")\n",
    "    b = json.load(a)\n",
    "    correct_ids = []\n",
    "    for k in b:\n",
    "        if k['reward'] == 1:\n",
    "            correct_ids.append(k['question_id'])\n",
    "    return correct_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_over_leaf_format(log_path, file_folder):\n",
    "    file_template = f\"{log_path}/{file_folder}\" + \"/agent_incident_{0}.json\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "    total_success_count = 0\n",
    "    total_cost = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "    total_round = 0\n",
    "\n",
    "    accs_str = \"\"\n",
    "\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "    for i in incidents:\n",
    "        print(f\"Analysis for incident {i}\")\n",
    "        with open(file_template.format(i), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        result = analysis(data, False)\n",
    "        accs_str += \"& \" + str(round(result['total_reward']/result['total_len'], 3)) + \" \"\n",
    "\n",
    "        total_count += result['total_len']\n",
    "        total_reward += result['total_reward']\n",
    "        total_success_count += result['success_count']\n",
    "        total_cost += result['total_cost']\n",
    "        total_prompt_tokens += result['total_prompt_tokens']\n",
    "        total_completion_tokens += result['total_completion_tokens']\n",
    "        total_round += result['total_round']\n",
    "    \n",
    "    accs_str += \"& \" + str(round(total_reward/total_count, 3)) + \" \"\n",
    "    accs_str += \"& \" + str(round(total_cost/total_count, 3)) + \" \"\n",
    "    accs_str += \"& \" + str(round(total_round/total_count)) + \" //\"\n",
    "    print(accs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaselineAgent_gpt-4o_c77_log_level_t0', 'PromptSauceAgent_gpt-4o_c72_alert_level_t0', 'BaselineAgent_4o-mini_c78_log_level_t0', 'PromptSauceAgent_gpt-4o_c75_log_level_t0', 'PromptSauceAgent_4o-mini_c74_log_level_t0', 'BaselineAgent_4o-mini_c71_alert_level_t0', 'PromptSauceAgent_4o-mini_c73_alert_level_t0', 'BaselineAgent_gpt-4o_c70_alert_level_t0']\n",
      "PromptSauceAgent_gpt-4o_c72_alert_level_t0\n",
      "Analysis for incident 5\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 34\n",
      "Max round: 25, Min round: 1\n",
      "Analysis for incident 38\n",
      "Max round: 25, Min round: 6\n",
      "Analysis for incident 39\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 55\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 134\n",
      "Max round: 25, Min round: 5\n",
      "Analysis for incident 166\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 322\n",
      "Max round: 25, Min round: 5\n",
      "& 0.364 & 0.506 & 0.333 & 0.366 & 0.222 & 0.328 & 0.31 & 0.385 & 0.351 & 0.553 & 14 //\n",
      "****************************************\n",
      "BaselineAgent_4o-mini_c71_alert_level_t0\n",
      "Analysis for incident 5\n",
      "Max round: 23, Min round: 3\n",
      "Analysis for incident 34\n",
      "Max round: 25, Min round: 1\n",
      "Analysis for incident 38\n",
      "Max round: 21, Min round: 4\n",
      "Analysis for incident 39\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 55\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 134\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 166\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 322\n",
      "Max round: 25, Min round: 4\n",
      "& 0.194 & 0.209 & 0.25 & 0.197 & 0.184 & 0.241 & 0.179 & 0.386 & 0.217 & 0.009 & 13 //\n",
      "****************************************\n",
      "PromptSauceAgent_4o-mini_c73_alert_level_t0\n",
      "Analysis for incident 5\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 34\n",
      "Max round: 25, Min round: 1\n",
      "Analysis for incident 38\n",
      "Max round: 25, Min round: 6\n",
      "Analysis for incident 39\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 55\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 134\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 166\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 322\n",
      "Max round: 25, Min round: 4\n",
      "& 0.288 & 0.362 & 0.367 & 0.267 & 0.173 & 0.345 & 0.338 & 0.34 & 0.295 & 0.016 & 13 //\n",
      "****************************************\n",
      "BaselineAgent_gpt-4o_c70_alert_level_t0\n",
      "Analysis for incident 5\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 34\n",
      "Max round: 25, Min round: 1\n",
      "Analysis for incident 38\n",
      "Max round: 25, Min round: 4\n",
      "Analysis for incident 39\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 55\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 134\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 166\n",
      "Max round: 25, Min round: 3\n",
      "Analysis for incident 322\n",
      "Max round: 25, Min round: 3\n",
      "& 0.37 & 0.334 & 0.25 & 0.273 & 0.215 & 0.483 & 0.172 & 0.333 & 0.299 & 0.465 & 14 //\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "dirs = os.listdir(\"../secgym/final_results\")\n",
    "print(dirs)\n",
    "for d in dirs:\n",
    "    # check if directory\n",
    "    if not \"alert_level_\" in d:\n",
    "        continue\n",
    "    print(d)\n",
    "    get_over_leaf_format(log_path, d)\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../secgym/final_results/BaselineAgent_gpt-4o_c70_alert_level_t0/agent_incident_5.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m incidents:\n\u001b[1;32m      8\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;241m.\u001b[39mformat(i)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m     result \u001b[38;5;241m=\u001b[39m analysis(data, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/secrl/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../secgym/final_results/BaselineAgent_gpt-4o_c70_alert_level_t0/agent_incident_5.json'"
     ]
    }
   ],
   "source": [
    "incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "\n",
    "file_name = \"BaselineAgent_gpt-4o_c70_alert_level_t0/agent_incident_{0}.json\"\n",
    "\n",
    "for i in incidents:\n",
    "    file_path = f\"{log_path}/{file_name.format(i)}\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    result = analysis(data, verbose=False)\n",
    "    print_analysis(result, head=f\"Analysis for incident {i}\")\n",
    "\n",
    "# analysis_one_run(log_path, file_name, print_total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max round: 25, Min round: 4\n",
      "****************************************\n",
      "Analysis for incident 55 analysis\n",
      "Average reward: 22.199999999999996/100 = 0.222\n",
      "Success rate: 21/100 = 21.0%\n",
      "** Computational Analysis:\n",
      "Average round: 1403/100 = 14.03\n",
      "Average cost: 71.18461499999998/100 = 0.7118\n",
      "Average prompt tokens: 13779234/100 = 137792.34\n",
      "Average completion tokens: 152563/100 = 1525.63\n",
      "** Query Efficiency: \n",
      "Success Query rate: 1037/1327 = 78.15%\n",
      "Success Non-Empty Query rate: 601/1327 = 45.29%\n",
      "Potential Good Example Count: 10 / 100\n",
      "Fail to run count: 0\n",
      "Max round: 25, Min round: 4\n",
      "****************************************\n",
      "Analysis for incident 5 analysis\n",
      "Average reward: 36.36/100 = 0.3636\n",
      "Success rate: 35/100 = 35.0%\n",
      "** Computational Analysis:\n",
      "Average round: 1417/100 = 14.17\n",
      "Average cost: 51.488265/100 = 0.5149\n",
      "Average prompt tokens: 9842961/100 = 98429.61\n",
      "Average completion tokens: 151564/100 = 1515.64\n",
      "** Query Efficiency: \n",
      "Success Query rate: 1089/1344 = 81.03%\n",
      "Success Non-Empty Query rate: 531/1344 = 39.51%\n",
      "Potential Good Example Count: 1 / 100\n",
      "Fail to run count: 0\n",
      "Max round: 25, Min round: 1\n",
      "****************************************\n",
      "Analysis for incident 34 analysis\n",
      "Average reward: 43/85 = 0.5059\n",
      "Success rate: 43/85 = 50.59%\n",
      "** Computational Analysis:\n",
      "Average round: 1142/85 = 13.4353\n",
      "Average cost: 55.10834500000001/85 = 0.6483\n",
      "Average prompt tokens: 10579100/85 = 124460.0\n",
      "Average completion tokens: 147523/85 = 1735.5647\n",
      "** Query Efficiency: \n",
      "Success Query rate: 905/1081 = 83.72%\n",
      "Success Non-Empty Query rate: 473/1081 = 43.76%\n",
      "Potential Good Example Count: 4 / 85\n",
      "Fail to run count: 0\n",
      "Max round: 25, Min round: 6\n",
      "****************************************\n",
      "Analysis for incident 38 analysis\n",
      "Average reward: 4/12 = 0.3333\n",
      "Success rate: 4/12 = 33.33%\n",
      "** Computational Analysis:\n",
      "Average round: 178/12 = 14.8333\n",
      "Average cost: 4.680175/12 = 0.39\n",
      "Average prompt tokens: 868109/12 = 72342.4167\n",
      "Average completion tokens: 22642/12 = 1886.8333\n",
      "** Query Efficiency: \n",
      "Success Query rate: 131/170 = 77.06%\n",
      "Success Non-Empty Query rate: 70/170 = 41.18%\n",
      "Potential Good Example Count: 0 / 12\n",
      "Fail to run count: 0\n",
      "Max round: 25, Min round: 5\n",
      "****************************************\n",
      "Analysis for incident 134 analysis\n",
      "Average reward: 19/58 = 0.3276\n",
      "Success rate: 19/58 = 32.76%\n",
      "** Computational Analysis:\n",
      "Average round: 906/58 = 15.6207\n",
      "Average cost: 29.362209999999994/58 = 0.5062\n",
      "Average prompt tokens: 5604371/58 = 96627.0862\n",
      "Average completion tokens: 89357/58 = 1540.6379\n",
      "** Query Efficiency: \n",
      "Success Query rate: 667/865 = 77.11%\n",
      "Success Non-Empty Query rate: 385/865 = 44.51%\n",
      "Potential Good Example Count: 4 / 58\n",
      "Fail to run count: 0\n",
      "Max round: 25, Min round: 3\n",
      "****************************************\n",
      "Analysis for incident 166 analysis\n",
      "Average reward: 27/87 = 0.3103\n",
      "Success rate: 27/87 = 31.03%\n",
      "** Computational Analysis:\n",
      "Average round: 1480/87 = 17.0115\n",
      "Average cost: 40.731800000000035/87 = 0.4682\n",
      "Average prompt tokens: 7744240/87 = 89014.2529\n",
      "Average completion tokens: 134040/87 = 1540.6897\n",
      "** Query Efficiency: \n",
      "Success Query rate: 1163/1423 = 81.73%\n",
      "Success Non-Empty Query rate: 601/1423 = 42.23%\n",
      "Potential Good Example Count: 4 / 87\n",
      "Fail to run count: 0\n",
      "no usage summary 9\n",
      "Max round: 25, Min round: 4\n",
      "****************************************\n",
      "Analysis for incident 39 analysis\n",
      "Average reward: 36.559999999999995/100 = 0.3656\n",
      "Success rate: 36/100 = 36.0%\n",
      "** Computational Analysis:\n",
      "Average round: 1376/100 = 13.76\n",
      "Average cost: 53.11833/100 = 0.5312\n",
      "Average prompt tokens: 9995322/100 = 99953.22\n",
      "Average completion tokens: 209448/100 = 2094.48\n",
      "** Query Efficiency: \n",
      "Success Query rate: 1023/1284 = 79.67%\n",
      "Success Non-Empty Query rate: 522/1284 = 40.65%\n",
      "Potential Good Example Count: 4 / 100\n",
      "Fail to run count: 1\n",
      "Max round: 25, Min round: 5\n",
      "****************************************\n",
      "Analysis for incident 322 analysis\n",
      "Average reward: 21.96/57 = 0.3853\n",
      "Success rate: 21/57 = 36.84%\n",
      "** Computational Analysis:\n",
      "Average round: 778/57 = 13.6491\n",
      "Average cost: 25.444875000000003/57 = 0.4464\n",
      "Average prompt tokens: 4837812/57 = 84873.8947\n",
      "Average completion tokens: 83721/57 = 1468.7895\n",
      "** Query Efficiency: \n",
      "Success Query rate: 554/733 = 75.58%\n",
      "Success Non-Empty Query rate: 338/733 = 46.11%\n",
      "Potential Good Example Count: 3 / 57\n",
      "Fail to run count: 0\n",
      "599\n",
      "no usage summary 117\n",
      "Max round: 25, Min round: 1\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 599\n",
      "Total reward: 210.08\n",
      "Total round: 8680\n",
      "Average reward: 210.08/599 = 0.350718\n",
      "Average success rate: 206/599 = 0.343907\n",
      "Average round: 8680/599 = 14.490818\n",
      "Difficulty 1: 19/46 = 0.413043\n",
      "Difficulty 3: 154.2/422 = 0.365403\n",
      "Difficulty 5: 24.88/98 = 0.253878\n",
      "Difficulty 7: 12/31 = 0.387097\n",
      "Difficulty 9: 0/1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "\n",
    "\n",
    "file_name = \"PromptSauceAgent_gpt-4o_c72_alert_level_t0_s25_trial1/agent_incident_{0}.json\"\n",
    "\n",
    "for i in incidents:\n",
    "    file_path = f\"{log_path}/{file_name.format(i)}\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    result = analysis(data, verbose=False)\n",
    "    print_analysis(result, head=f\"Analysis for incident {i}\")\n",
    "\n",
    "analysis_one_run(log_path, file_name, print_total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'nodes': '48-108', 'reward': 0, 'question_dict': {'context': \"In the context of a security incident, a new user account 'bdadmin' with specific details was created using a file `net.exe` to maintain access to the device or organization. The command was executed using a `powershell.exe` script located at `c:\\\\users\\\\tgs2z\\\\downloads\\\\domaindominance27\\\\domaindominance27.ps1`. This incident highlights that attackers might be attempting to sustain their access.\", 'question': 'In the same device and timeframe, there was a detection of a credential theft tool. Can you identify the name of the file used for this credential theft?', 'answer': 'mimikatz.exe', 'solution': ['A suspicious new user account creation was initiated by `powershell.exe`.', 'The Mimikatz credential theft tool `mimikatz.exe` was detected, potentially being used to harvest credentials on the device.'], 'start_alert': 48, 'end_alert': 108, 'start_entities': [18, 49, 50, 51], 'end_entities': [64], 'shortest_alert_path': [48, 17, 108]}, 'trial': 0, 'info': {}, 'messages': [{'role': 'system', 'content': 'You are a security analyst. \\nYou need to answer a given security question by querying the database.\\nThe logs are stored in a MySQL database, you can use SQL queries to retrieve entries as needed.\\nNote there are more than 20 tables in the database, so you may need to explore the schema or check example entries to understand the database structure.\\n\\nYour response should always be a thought-action pair:\\nThought: <your reasoning>\\nAction: <your SQL query>\\n\\nIn Thought, you can analyse and reason about the current situation, \\nAction can be one of the following: \\n(1) execute[<your query>], which executes the SQL query\\n(2) submit[<your answer>], which is the final answer to the question\\n'}, {'role': 'user', 'content': \"In the context of a security incident, a new user account 'bdadmin' with specific details was created using a file `net.exe` to maintain access to the device or organization. The command was executed using a `powershell.exe` script located at `c:\\\\users\\\\tgs2z\\\\downloads\\\\domaindominance27\\\\domaindominance27.ps1`. This incident highlights that attackers might be attempting to sustain their access. In the same device and timeframe, there was a detection of a credential theft tool. Can you identify the name of the file used for this credential theft?\"}], 'usage_summary': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "log_path = \"C:/Users/amudgerikar/source/repos/SecRL/secgym/results/\"\n",
    "file_template = \"PromptSauceAgent_incident_166_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "#incidents = [5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 207.32000000000005\n",
      "Total round: 7013\n",
      "Average reward: 207.32000000000005/676 = 0.306686\n",
      "Average round: 7013/676 = 10.37426\n",
      "Difficulty 1: 27.16/66 = 0.411515\n",
      "Difficulty 3: 115.6/402 = 0.287562\n",
      "Difficulty 5: 42.16/127 = 0.331969\n",
      "Difficulty 7: 17.4/71 = 0.24507\n",
      "Difficulty 9: 5/10 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/base_agent_experiments_4o/alert_level\"\n",
    "file_template = \"incident_{}_agent_log_gpt-4o_46_alert.json\"\n",
    "\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 344.864\n",
      "Total round: 5846\n",
      "Average reward: 344.864/676 = 0.510154\n",
      "Average round: 5846/676 = 8.647929\n",
      "Difficulty 1: 39/66 = 0.590909\n",
      "Difficulty 3: 221.4/402 = 0.550746\n",
      "Difficulty 5: 57/127 = 0.448819\n",
      "Difficulty 7: 26.46/71 = 0.372732\n",
      "Difficulty 9: 1/10 = 0.1\n"
     ]
    }
   ],
   "source": [
    "# reflexion + GPT-4o\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/reflexion_agent_experiments_4o/alert_level/steps=15\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Sauce Agent GPT-4o\n",
      "Step = 15\n",
      "676\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 676\n",
      "Total reward: 270.18399999999997\n",
      "Total round: 6978\n",
      "Average reward: 270.18399999999997/676 = 0.39968\n",
      "Average round: 6978/676 = 10.322485\n",
      "Difficulty 1: 26/66 = 0.393939\n",
      "Difficulty 3: 161.6/402 = 0.40199\n",
      "Difficulty 5: 51.12/127 = 0.40252\n",
      "Difficulty 7: 28.46/71 = 0.400901\n",
      "Difficulty 9: 3/10 = 0.3\n",
      "Step = 50\n",
      "654\n",
      "****************************************\n",
      "****************************************\n",
      "Total analysis\n",
      "Total length: 654\n",
      "Total reward: 321.66399999999993\n",
      "Total round: 10638\n",
      "Average reward: 321.66399999999993/654 = 0.491841\n",
      "Average round: 10638/654 = 16.266055\n",
      "Difficulty 1: 33.72/66 = 0.510909\n",
      "Difficulty 3: 196.36/380 = 0.516737\n",
      "Difficulty 5: 51.72/127 = 0.407244\n",
      "Difficulty 7: 34.86/71 = 0.491042\n",
      "Difficulty 9: 5/10 = 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt Sauce Agent GPT-4o\")\n",
    "\n",
    "print(\"Step = 15\")\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/prompt_sauce_agent_experiments_4o/alert_level/steps=15\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_46_alert_sum.json\"\n",
    "analysis_one_run(path, file_template, True)\n",
    "\n",
    "\n",
    "print(\"Step = 50\")\n",
    "path = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/prompt_sauce_agent_experiments_4o/alert_level/steps=50\"\n",
    "file_template = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_46_alert_sum.json\"\n",
    "analysis_one_run(path, file_template, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method                   | Accuracy (%) |\n",
    "|--------------------------|----------|\n",
    "| Baseline                | 30.7     |\n",
    "| Refined Prompt          | 40.0     |\n",
    "| Refined Prompt (step=50)| 49.2     |\n",
    "| Reflexion               | 51.0      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap of correct questions with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_correct_problem_ids(log_path, file_template, incident_id):\n",
    "    with open(f\"{log_path}/{file_template.format(incident_id)}\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    correct_ids = []\n",
    "    for i, entry in enumerate(data):\n",
    "        if entry['reward'] == 1:\n",
    "            correct_ids.append(i)\n",
    "    return correct_ids\n",
    "\n",
    "def jaccard_similarity(path_1, template_1, path_2, template_2, incidents):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard similarity between two agents across multiple incidents.\n",
    "\n",
    "    Args:\n",
    "        path_1 (str): Path to logs for Agent 1.\n",
    "        template_1 (str): File template for Agent 1's logs.\n",
    "        path_2 (str): Path to logs for Agent 2.\n",
    "        template_2 (str): File template for Agent 2's logs.\n",
    "        incidents (list of int): List of incident IDs to compare.\n",
    "\n",
    "    Returns:\n",
    "        float: The overall Jaccard similarity across all incidents.\n",
    "    \"\"\"\n",
    "    union_set = set()\n",
    "    intersection_set = set()\n",
    "\n",
    "    offset = 0  # Initialize offset to ensure unique IDs across incidents\n",
    "\n",
    "    for incident_id in incidents:\n",
    "        # Retrieve correct question IDs for each agent and apply offset\n",
    "        correct_1 = {qid + offset for qid in get_correct_problem_ids(path_1, template_1, incident_id)}\n",
    "        correct_2 = {qid + offset for qid in get_correct_problem_ids(path_2, template_2, incident_id)}\n",
    "\n",
    "        print(f\"Incident {incident_id}\")\n",
    "        print(f\"Correct 1: {correct_1}\")\n",
    "        print(f\"Correct 2: {correct_2}\")\n",
    "        print()\n",
    "        \n",
    "        # Update union and intersection sets\n",
    "        union_set.update(correct_1 | correct_2)\n",
    "        intersection_set.update(correct_1 & correct_2)\n",
    "        \n",
    "        # Increment offset to avoid ID conflicts for the next incident\n",
    "        offset += max(len(correct_1), len(correct_2)) + 1\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    if not union_set:  # To handle cases with no data\n",
    "        return 0.0\n",
    "    return len(intersection_set) / len(union_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident 55\n",
      "Correct 1: {0, 4, 8, 14, 21, 24, 26, 29, 30, 31, 32, 38, 39, 41, 46, 47, 51, 54, 57, 58, 59, 61, 70, 78, 79, 86, 93, 94, 96, 98, 99}\n",
      "Correct 2: {0, 4, 7, 8, 11, 13, 14, 22, 23, 24, 28, 30, 31, 32, 34, 35, 38, 43, 44, 47, 48, 51, 53, 56, 57, 62, 67, 69, 75, 76, 78, 81, 83, 98}\n",
      "\n",
      "Incident 5\n",
      "Correct 1: {133, 134, 36, 37, 38, 39, 52, 54, 58, 60, 62, 66, 67, 68, 69, 73, 74, 76, 87, 94, 95, 97, 99, 100, 103, 112, 114, 117, 119, 123, 124, 125}\n",
      "Correct 2: {130, 131, 133, 35, 36, 37, 38, 39, 40, 43, 45, 47, 52, 53, 54, 56, 62, 65, 67, 68, 73, 75, 89, 90, 91, 92, 93, 94, 95, 98, 99, 103, 110, 111, 112, 113, 114, 118, 119, 124, 125}\n",
      "\n",
      "Incident 34\n",
      "Correct 1: {129, 131, 132, 133, 143, 163, 166, 169, 79, 80, 81, 82, 84, 91, 92, 94, 103, 104, 112, 114, 116, 125}\n",
      "Correct 2: {129, 130, 131, 132, 133, 135, 136, 139, 140, 143, 144, 146, 147, 148, 152, 153, 154, 160, 163, 164, 165, 166, 168, 169, 170, 172, 175, 176, 77, 78, 79, 81, 84, 91, 92, 93, 94, 96, 97, 98, 105, 106, 108, 110, 111, 113, 114, 115, 116, 117, 120, 121, 122, 125, 126, 127}\n",
      "\n",
      "Incident 38\n",
      "Correct 1: {146, 140, 141}\n",
      "Correct 2: {135, 138, 139, 140, 143, 144, 146, 147, 149}\n",
      "\n",
      "Incident 134\n",
      "Correct 1: {149, 150, 151, 152, 154, 159, 162, 166, 171, 174, 175, 176, 185, 187, 188, 189, 194, 195, 196, 199, 201, 205, 206, 218, 219, 224}\n",
      "Correct 2: {144, 145, 147, 148, 150, 152, 154, 155, 156, 158, 159, 160, 162, 164, 166, 167, 168, 171, 172, 173, 174, 175, 178, 179, 180, 185, 186, 188, 189, 190, 192, 194, 195, 196, 197, 198, 200, 201, 205, 207, 208, 209, 211, 216, 219, 220, 224}\n",
      "\n",
      "Incident 166\n",
      "Correct 1: {257, 261, 263, 268, 273, 274, 277, 280, 281, 284, 285, 289, 194, 195, 196, 197, 199, 203, 206, 207, 218, 219, 222, 223, 227, 233, 246, 254}\n",
      "Correct 2: {256, 257, 261, 263, 265, 268, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 193, 194, 195, 197, 199, 203, 204, 205, 206, 207, 210, 211, 213, 215, 218, 219, 220, 222, 223, 227, 233, 236, 238, 239, 242, 246, 252, 253, 254}\n",
      "\n",
      "Incident 39\n",
      "Correct 1: {256, 257, 258, 260, 261, 263, 264, 265, 274, 277, 278, 283, 285, 286, 287, 291, 292, 295, 300, 308, 310, 319, 321, 322, 327, 328, 338, 245, 250, 253}\n",
      "Correct 2: {256, 257, 258, 260, 261, 262, 264, 265, 267, 268, 269, 272, 274, 275, 277, 278, 279, 280, 283, 284, 286, 287, 288, 290, 292, 294, 295, 300, 301, 304, 308, 309, 310, 311, 319, 321, 322, 324, 326, 328, 333, 338, 340, 248, 249, 250, 252, 253, 254}\n",
      "\n",
      "Incident 322\n",
      "Correct 1: {296, 297, 299, 301, 302, 304, 305, 308, 309, 310, 312, 313, 314, 318, 319, 320, 321, 327, 329, 331, 337, 340, 341, 343, 346, 348, 349, 354, 357, 363, 371}\n",
      "Correct 2: {295, 296, 297, 299, 301, 302, 303, 304, 305, 308, 310, 311, 312, 313, 314, 315, 318, 319, 320, 321, 322, 323, 327, 328, 329, 330, 331, 333, 334, 337, 338, 339, 340, 341, 343, 346, 348, 349, 350, 351, 354, 355, 357, 358, 359, 363, 364, 365, 366, 367, 368, 371, 372, 373}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4755244755244755"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "path_1 = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/base_agent_experiments_4o/alert_level\"\n",
    "template_1 = \"incident_{}_agent_log_gpt-4o_46_alert.json\"\n",
    "\n",
    "# Reflexion\n",
    "path_2 = \"/Users/kevin/Downloads/SecRL/secgym/agent_experiment_logs/reflexion_agent_experiments_4o/alert_level/steps=15\"\n",
    "template_2 = \"PromptSauceAgent_incident_{}_agent_log_gpt-4o_111_alert.json\"\n",
    "\n",
    "incidents = [55, 5, 34, 38, 134, 166, 39, 322]\n",
    "\n",
    "jaccard_similarity(path_1=path_1, template_1=template_1, path_2=path_2, template_2=template_2, incidents=incidents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### GPT-4o-mini\n",
    "\n",
    "| Incident Number  | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|--------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| Alert Counted (Sorted by) | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "| Log              | 0.081   | 0.137   | 0.099  | 0.278   | 0.254   | 0.215   | 0.318    | 0.025   |\n",
    "| Alert            | 0.111   | 0.218   | 0.150  | 0.264   | 0.236   | 0.252   | 0.316    | 0.088   |\n",
    "| Incident         | 0.094   | 0.144   | 0.214  | 0.318   | 0.206   | 0.254   | 0.253    | 0.063   |\n",
    "\n",
    "\n",
    "<!-- \n",
    "### GPT-4o Log Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "| **Average Reward**  | 0.070   | 0.186   | 0.172  | 0.194   | 0.190   | 0.257   | 0.385    | 0.125   |\n",
    "| **Average Round**   | 12.08   | 10.88   | 11.6   | 11.52   | 11.13   | 10.667  | 9.987    | 11.375  |\n",
    "\n",
    "    total_len += result_dict['total_len']\n",
    "    total_reward += result_dict['total_reward']\n",
    "    total_round += result_dict['total_round']\n",
    "    \n",
    "    for k in result_dict['path_count']:\n",
    "        if k not in path_count:\n",
    "            path_count[k] = 0\n",
    "            reward_count[k] = 0\n",
    "\n",
    "### GPT-4o Incident Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.234   | 0.287   | 0.262  | 0.218   | 0.314   | 0.331   | 0.435    | 0.188   |\n",
    "| **Average Round**   | 10.05   | 9.98    | 10.27  | 10.87   | 10.39   | 10.370  | 9.684    | 11.688  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "\n",
    "### GPT-4o-mini Log Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.081   | 0.137   | 0.099  | 0.278   | 0.254   | 0.215   | 0.318    | 0.025   |\n",
    "| **Average Round**   | 12.24   | 12.28   | 11.7   | 10.83   | 10.85   | 11.469  | 11.405   | 11.438  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "### GPT-4o-mini Alert Level\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.111   | 0.218   | 0.150  | 0.264   | 0.236   | 0.252   | 0.316    | 0.088   |\n",
    "| **Average Round**   | 11.41   | 11.92   | 11.84  | 10.82   | 10.9    | 11.123  | 10.937   | 11.188  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       |\n",
    "\n",
    "### GPT-4o-mini Incident Level\n",
    "\n",
    "| **Metric**         | **5**   | **39**  | **55** | **34**  | **166** | **134** | **322**  | **38**  |\n",
    "|---------------------|---------|---------|--------|---------|---------|---------|----------|---------|\n",
    "| **Average Reward**  | 0.094   | 0.144   | 0.214  | 0.318   | 0.206   | 0.254   | 0.253    | 0.063   |\n",
    "| **Average Round**   | 11.58   | 11.92   | 11.54  | 10.84   | 10.89   | 11.309  | 11.025   | 10.313  |\n",
    "| **Alert Number**    | 68      | 47      | 17     | 11      | 11      | 9       | 9        | 4       | -->\n",
    "\n",
    "# Difficulty Reward Change Table\n",
    "\n",
    "<!-- | **Model**    | **Level**       | **Reward Change (1 → 3 → 5 → 7 → 9)**      | **Avg Reward** | **Avg Round** |\n",
    "|--------------|-----------------|---------------------------------------------|---------------|---------------|\n",
    "| GPT-4o       | Log Level       | 0.326667 → 0.192438 → 0.181102 → 0.132169 → 0.3 | 0.198675      | 11.177515     |\n",
    "| GPT-4o-mini  | Log Level       | 0.35697 → 0.201493 → 0.143307 → 0.071324 → 0.0  | 0.189089      | 11.542899     |\n",
    "| GPT-4o       | Alert Level     | 0.411515 → 0.287562 → 0.331969 → 0.24507 → 0.5  | 0.306686      | 10.37426      |\n",
    "| GPT-4o-mini  | Alert Level     | 0.341818 → 0.210945 → 0.205669 → 0.155831 → 0.0 | 0.213822      | 11.29142      |\n",
    "| GPT-4o       | Incident Level  | 0.366061 → 0.273632 → 0.308346 → 0.258028 → 0.4 | 0.289408      | 10.278107     |\n",
    "| GPT-4o-mini  | Incident Level  | 0.320606 → 0.20995 → 0.154016 → 0.169014 → 0.2  | 0.205799      | 11.285503     | -->\n",
    "\n",
    "| **Model**    | **Level**       | **Reward Change (1 → 3 → 5 → 7 → 9)**      | **Avg Reward** | **Avg Round** |\n",
    "|--------------|-----------------|---------------------------------------------|---------------|---------------|\n",
    "| GPT-4o       | Log Level       | 0.327 → 0.192 → 0.181 → 0.132 → 0.300      | 0.199         | 11.178        |\n",
    "| GPT-4o-mini  | Log Level       | 0.357 → 0.201 → 0.143 → 0.071 → 0.000      | 0.189         | 11.543        |\n",
    "| GPT-4o       | Alert Level     | 0.412 → 0.288 → 0.332 → 0.245 → 0.500      | 0.307         | 10.374        |\n",
    "| GPT-4o-mini  | Alert Level     | 0.342 → 0.211 → 0.206 → 0.156 → 0.000      | 0.214         | 11.291        |\n",
    "| GPT-4o       | Incident Level  | 0.366 → 0.274 → 0.308 → 0.258 → 0.400      | 0.289         | 10.278        |\n",
    "| GPT-4o-mini  | Incident Level  | 0.321 → 0.210 → 0.154 → 0.169 → 0.200      | 0.206         | 11.286        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Validation test\n",
    "\n",
    "1. With higher difficulty, the reward should be lower.\n",
    "2. More advanced model should have higher reward.\n",
    "3. Are the correct questions always the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello {nmae}, how are you doing today?\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nmae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m dedent(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mhello \u001b[39m\u001b[38;5;132;01m{nmae}\u001b[39;00m\u001b[38;5;124m, how are you doing today?\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKevin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nmae'"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "prompt = dedent(\"\"\"hello {nmae}, how are you doing today?\"\"\")\n",
    "print(prompt)\n",
    "\n",
    "print(prompt.format(\"Kevin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
