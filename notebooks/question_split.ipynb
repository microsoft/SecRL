{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secgym.qagen.alert_graph import AlertGraph\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "def compute_overlap_score(path1, path2, alpha=3, beta=1):\n",
    "    \"\"\"\n",
    "    Calculate the overlap score between two paths based on shared and unshared edges.\n",
    "\n",
    "    Parameters:\n",
    "        path (list): The first path as a sequence of nodes.\n",
    "        other_path (list): The second path as a sequence of nodes.\n",
    "        alpha (float): Weight for shared edges (positive contribution).\n",
    "        beta (float): Weight for unshared edges (negative contribution).\n",
    "\n",
    "    Returns:\n",
    "        float: Overlap score.\n",
    "    \"\"\"\n",
    "    if len(path1) <= 1 or len(path2) <= 1:\n",
    "        return 0\n",
    "    # Get edges for both paths\n",
    "    edges1 = set((path1[i], path1[i + 1]) for i in range(len(path1) - 1))\n",
    "    edges2 = set((path2[i], path2[i + 1]) for i in range(len(path2) - 1))\n",
    "    \n",
    "    # Shared and unshared edges\n",
    "    shared_edges = edges1 & edges2\n",
    "    if len(shared_edges) == 0:\n",
    "        return 0\n",
    "    unshared_edges = edges1 ^ edges2 \n",
    "\n",
    "    # Compute score\n",
    "    score = alpha * len(shared_edges) - beta * len(unshared_edges) / (len(edges1) + len(edges2))\n",
    "    # bound: [-beta, alpha]\n",
    "    # resacle to [0, 1]\n",
    "    # score = (score + beta) / (alpha + beta)\n",
    " \n",
    "    return score\n",
    "\n",
    "def split_train_test(all_paths:dict, train_ratio:float = 0.9, trials=5):\n",
    "    \"\"\"\n",
    "    all_path = [{'start_alert': int, 'end_alert': int}, ...]\n",
    "\n",
    "    {\n",
    "        'start_alert': 24,\n",
    "        'end_alert': 11,\n",
    "        'start_entities': [12],\n",
    "        'end_entities': [9],\n",
    "        'shortest_alert_path': [24, 9, 11]\n",
    "    }\n",
    "    \"\"\"\n",
    "    # construct a dictionary to map the path to the original dictionary\n",
    "    random.shuffle(all_paths)\n",
    "    path_to_dict = {}\n",
    "    for path in all_paths:\n",
    "        # extract the 0, 2, 4, 6, ... index of the path\n",
    "        extracted = path['shortest_alert_path'][::2]\n",
    "        path_to_dict[tuple(extracted)] = path\n",
    "    \n",
    "    # prepare the alert paths\n",
    "    alert_paths = list(path_to_dict.keys())\n",
    "    max_length = max(len(p) for p in alert_paths)\n",
    "    total = len(alert_paths)\n",
    "    train_len = int(total * train_ratio)\n",
    "    test_len = total - train_len\n",
    "\n",
    "    # length weight\n",
    "    lweights = []\n",
    "    for p in alert_paths:\n",
    "        lweights.append(len(p) / max_length)\n",
    "    lweights = np.array(lweights)\n",
    "    lweights = lweights / lweights.sum()\n",
    "    avg = lweights.mean()\n",
    "\n",
    "    score_matrix = [[-1000] * len(alert_paths)  for _ in range(len(alert_paths))]\n",
    "\n",
    "    # helper function to get the score\n",
    "    def get_score(i, j):\n",
    "        if score_matrix[i][j] != -1000:\n",
    "            return score_matrix[i][j]\n",
    "        if i==j:\n",
    "            score_matrix[i][j] = 0\n",
    "            return 0\n",
    "        elif score_matrix[j][i] != -1000:\n",
    "            score_matrix[i][j] = score_matrix[j][i]\n",
    "            return score_matrix[i][j]\n",
    "        score_matrix[i][j] = compute_overlap_score(alert_paths[i], alert_paths[j])\n",
    "        return score_matrix[i][j]\n",
    "\n",
    "    # random split and compare\n",
    "    train_keys = []\n",
    "    test_keys = []\n",
    "    score_splits = {}\n",
    "    for _ in range(trials):\n",
    "        for i in range(total):\n",
    "            if len(train_keys) >= train_len:\n",
    "                test_keys += alert_paths[i:]\n",
    "                break\n",
    "            elif len(test_keys) >= test_len:\n",
    "                train_keys += alert_paths[i:]\n",
    "                break\n",
    "            if random.random() > (lweights[i] / (lweights[i] + avg)):\n",
    "                train_keys.append(alert_paths[i])\n",
    "            else:\n",
    "                test_keys.append(alert_paths[i])\n",
    "\n",
    "        compare_score = 0\n",
    "        for k in train_keys:\n",
    "            for j in test_keys:\n",
    "                compare_score += get_score(alert_paths.index(k), alert_paths.index(j))\n",
    "\n",
    "        score_splits[compare_score] = (train_keys, test_keys)\n",
    "        train_keys = []\n",
    "        test_keys = []\n",
    "\n",
    "    # assert len(final_train_set) + len(final_test_set) == total, f\"Length mismatch: {len(final_train_set)} + {len(final_test_set)} != {total}\"\n",
    "    return score_splits, path_to_dict\n",
    "\n",
    "graph_path = \"/Users/kevin/Downloads/SecRL/secgym/qagen/graph_files\"\n",
    "qa_path = \"/Users/kevin/Downloads/SecRL/secgym/qagen/graph_path\"\n",
    "\n",
    "train_total_count = 0\n",
    "test_total_count = 0\n",
    "\n",
    "median_score_path = \"./media_split\"\n",
    "high_score_path = \"./high_split\"\n",
    "low_score_path = \"./low_split\"\n",
    "\n",
    "# create\n",
    "os.makedirs(median_score_path, exist_ok=True)\n",
    "os.makedirs(high_score_path, exist_ok=True)\n",
    "os.makedirs(low_score_path, exist_ok=True)\n",
    "\n",
    "def save_to_split(path, filename, train_keys, test_keys, path_to_dict):\n",
    "    with open(os.path.join(path, filename.split(\".\")[0] + \".json\"), \"w\") as f:\n",
    "        train_set = [path_to_dict[p1] for p1 in train_keys]\n",
    "        test_set = [path_to_dict[p2] for p2 in test_keys]\n",
    "        json.dump({\"train\": train_set, \"test\": test_set}, f)\n",
    "\n",
    "for filename in os.listdir(graph_path):\n",
    "    if filename.endswith(\".graphml\"):\n",
    "        # if \"_5.\" in filename:\n",
    "        #     continue\n",
    "        print(filename)\n",
    "\n",
    "        graphfile = graph_path + \"/\" + filename\n",
    "        alert_graph = AlertGraph()\n",
    "        alert_graph.load_graph_from_graphml(graphfile)\n",
    "        all_paths = alert_graph.get_alert_paths(verbose=False)\n",
    "\n",
    "        if len(all_paths) < 150:\n",
    "            train_ratio = 0.288\n",
    "            # trials = 20\n",
    "        else:\n",
    "            train_ratio = 1 - 100 / len(all_paths)\n",
    "        print(\"Path length:\", len(all_paths), \"Train ratio:\", train_ratio)\n",
    "\n",
    "        score_splits, path_to_dict = split_train_test(all_paths, train_ratio, trials=50)\n",
    "\n",
    "        # save high, low, median score\n",
    "        scores = list(score_splits.keys())\n",
    "        scores.sort()\n",
    "        median = scores[len(scores) // 2]\n",
    "        high = scores[-1]\n",
    "        low = scores[0]\n",
    "\n",
    "        save_to_split(median_score_path, filename, *score_splits[median], path_to_dict)\n",
    "        save_to_split(high_score_path, filename, *score_splits[high], path_to_dict)\n",
    "        save_to_split(low_score_path, filename, *score_splits[low], path_to_dict)\n",
    "        \n",
    "        print(\"Median score:\", median, \"High score:\", high, \"Low score:\", low)\n",
    "    \n",
    "\n",
    "        # qafile = qa_path + \"/\" + filename.split(\".\")[0] + \".json\"\n",
    "        # if os.path.exists(qafile):\n",
    "        #     # get best score\n",
    "        #     with open(qafile, \"r\") as f:\n",
    "        #         data = json.load(f)\n",
    "        #         best_score = data[\"score\"]\n",
    "        #         print(\"New best score:\", score, \"Old best score:\", best_score)\n",
    "        #         if score > best_score:\n",
    "        #             print(\"New best score is better, update\")\n",
    "        #             with open(qafile, \"w\") as f:\n",
    "        #                 json.dump({\"train\": train_set, \"test\": test_set, \"score\": score}, f)\n",
    "        #         else:\n",
    "        #             print(\"Old best score is better, skip\")\n",
    "        # else:\n",
    "        #     with open(qafile, \"w\") as f:\n",
    "        #         json.dump({\"train\": train_set, \"test\": test_set, \"score\": score}, f)\n",
    "            \n",
    "        # print(\"Train set:\", len(train_set), \"Test set:\", len(test_set))\n",
    "        # train_total_count += len(train_set)\n",
    "        # test_total_count += len(test_set)\n",
    "        # print(\"-\"*100)\n",
    "\n",
    "print(\"Total test set:\", test_total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read low split test sample and count number of examples for each length of shortest_alert_path\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for filename in os.listdir(low_score_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(low_score_path, filename), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            test_set = data[\"test\"]\n",
    "            lengths = [len(p[\"shortest_alert_path\"]) for p in test_set]\n",
    "            # count of each length and print\n",
    "            count = {}\n",
    "            for l in lengths:\n",
    "                if l not in count:\n",
    "                    count[l] = 0\n",
    "                count[l] += 1\n",
    "\n",
    "            # sort by length\n",
    "            count = dict(sorted(count.items()))\n",
    "\n",
    "            print(f\"{filename}:\")\n",
    "            for k, v in count.items():\n",
    "                print(f\"Length {k}: {v} examples\")\n",
    "\n",
    "            print(\"============================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
