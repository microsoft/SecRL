{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "import json\n",
    "from anaysis import analysis, print_analysis, get_correct_problem_ids, get_over_leaf_format, analysis_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4o-mini v1\n",
      "589\n",
      "& 0.163 & 0.195 & 0.273 & 0.185 & 0.174 & 0.228 & 0.163 & 0.276 & 0.192 \n",
      "===================\n",
      "gpt-4o v1\n",
      "589\n",
      "& 0.338 & 0.293 & 0.364 & 0.273 & 0.249 & 0.491 & 0.166 & 0.315 & 0.293 \n",
      "===================\n",
      "o1-mini v2\n",
      "589\n",
      "& 0.147 & 0.244 & 0.091 & 0.23 & 0.16 & 0.333 & 0.189 & 0.382 & 0.222 \n",
      "===================\n",
      "o3-mini v2\n",
      "No usage summary, skipping 32-51, trial 0\n",
      "589\n",
      "& 0.35 & 0.293 & 0.273 & 0.257 & 0.227 & 0.404 & 0.253 & 0.36 & 0.296 \n",
      "===================\n",
      "phi-4 v2\n",
      "589\n",
      "& 0.086 & 0.037 & 0.182 & 0.082 & 0.066 & 0.13 & 0.085 & 0.125 & 0.085 \n",
      "===================\n",
      "r1 v2\n",
      "589\n",
      "& 0.106 & 0.09 & 0.0 & 0.169 & 0.043 & 0.025 & 0.074 & 0.054 & 0.084 \n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "baselines = {\n",
    "  \"gpt-4o-mini\": \"BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\",\n",
    "  \"gpt-4o\": \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\",\n",
    "  \"o1-mini\": (\"BaselineAgent_o1-mini_c92_alert_level_t0_s25_trial1\", \"v2\"),\n",
    "  \"o3-mini\": (\"BaselineAgent_o3-mini_c99_alert_level_t0_s25_trial1\", \"v2\"),\n",
    "  \"phi-4\": (\"BaselineAgent_phi4_c469_alert_level_t0_s25_trial1\", \"v2\"),\n",
    "  \"r1\": (\"BaselineAgent_r1_c468_alert_level_t0_s25_trial1\", \"v2\"),\n",
    "}\n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "\n",
    "for name, b in baselines.items():\n",
    "\n",
    "    if type(b) == tuple:\n",
    "        file_folder, version = b\n",
    "    else:\n",
    "        file_folder = b\n",
    "        version = \"v1\"\n",
    "    print(name, version)\n",
    "    get_over_leaf_format(log_path, file_folder, version)\n",
    "    print(\"===================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiModelBaselineAgent_master_o1_mini_slave_gpt-4o_c96_alert_level_t0_s25_trial1\n",
      "589\n",
      "& 0.304 & 0.256 & 0.273 & 0.238 & 0.296 & 0.316 & 0.211 & 0.379 & 0.279 \n",
      "MultiModelBaselineAgent_master_o1_slave_gpt-4o_c98_alert_level_t0_s25_trial1\n",
      "No usage summary, skipping 109-34, trial 0\n",
      "589\n",
      "& 0.398 & 0.317 & 0.091 & 0.265 & 0.297 & 0.474 & 0.228 & 0.391 & 0.323 \n",
      "MultiModelBaselineAgent_master_o3_mini_slave_gpt-4o_c100_alert_level_t0_s25_trial1\n",
      "589\n",
      "& 0.404 & 0.31 & 0.364 & 0.274 & 0.264 & 0.333 & 0.218 & 0.375 & 0.308 \n"
     ]
    }
   ],
   "source": [
    "mmbaselines = [\n",
    "    \"MultiModelBaselineAgent_master_o1_mini_slave_gpt-4o_c96_alert_level_t0_s25_trial1\",\n",
    "    \"MultiModelBaselineAgent_master_o1_slave_gpt-4o_c98_alert_level_t0_s25_trial1\",\n",
    "    \"MultiModelBaselineAgent_master_o3_mini_slave_gpt-4o_c100_alert_level_t0_s25_trial1\"\n",
    "]\n",
    "\n",
    "for b in mmbaselines:\n",
    "    print(b)\n",
    "    get_over_leaf_format(log_path, b, version=\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anaysis import analysis, add_to_usage, print_analysis, get_over_leaf_format, analysis_v2\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def add_to_usage(usage_summary:dict, total_cost=0, total_prompt_tokens=0, total_completion_tokens=0):\n",
    "    model = list(usage_summary.keys())[-1]\n",
    "    total_prompt_tokens += usage_summary[model]['prompt_tokens']\n",
    "    total_completion_tokens += usage_summary[model]['completion_tokens']\n",
    "    return total_cost, total_prompt_tokens, total_completion_tokens    \n",
    "\n",
    "def get_avg_accuracy_and_cost(log_path, file_folder, model_name, version=\"v1\", round_cut=-1):\n",
    "    d = 1_000_000  # price is per 1M tokens\n",
    "    prices = {\n",
    "        \"gpt-4o\": (2.5 / d, 10 / d),\n",
    "        \"gpt-4o-mini\": (0.15 / d, 0.6 / d),\n",
    "        \"o1-mini\": (1.1 / d, 4.4 / d),\n",
    "        \"o1\": (15 / d, 7.5 / d),\n",
    "        \"o3-mini\": (1.1 / d, 4.4 / d),\n",
    "        \"deepseek-r1\": (0.55 / d, 0.27 / d),\n",
    "        \"phi4\": (0.07/ d, 0.14/ d),\n",
    "    }\n",
    "\n",
    "    prompt_price, completion_price = prices[model_name]\n",
    "    file_template = f\"{log_path}/{file_folder}\" + \"/agent_incident_{0}.json\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "    total_round = 0\n",
    "\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "    for i in incidents:\n",
    "        with open(file_template.format(i), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if version == \"v2\":\n",
    "            result = analysis_v2(data, round_cut=round_cut)\n",
    "        else:\n",
    "            result = analysis(data, round_cut=round_cut)\n",
    "\n",
    "        total_count += result['total_len']\n",
    "        total_reward += result['total_reward']\n",
    "        total_prompt_tokens += result['total_prompt_tokens']\n",
    "        total_completion_tokens += result['total_completion_tokens']\n",
    "        total_round += result['total_round']\n",
    "\n",
    "    avg_accuracy = round(total_reward / total_count, 3)\n",
    "    avg_cost = round(\n",
    "        (total_prompt_tokens * prompt_price + total_completion_tokens * completion_price)\n",
    "        / total_count,\n",
    "        5,\n",
    "    )\n",
    "    avg_round = round(total_round / total_count, 3)\n",
    "\n",
    "    return avg_accuracy, avg_cost, avg_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-gpt-4o v1\n",
      "Avg Accuracy: 0.293, Avg Cost: 0.23517 Avg Round: 14.154\n",
      "& 0.293 & 14.154 & 0.23517 \n",
      "strategy-gpt-4o v2\n",
      "Error calculating usage: usage summary None\n",
      "Avg Accuracy: 0.273, Avg Cost: 0.17797 Avg Round: 10.555\n",
      "& 0.273 & 10.555 & 0.17797 \n",
      "expel-gpt-4o v2\n",
      "Avg Accuracy: 0.39, Avg Cost: 0.37938 Avg Round: 8.684\n",
      "& 0.39 & 8.684 & 0.37938 \n",
      "react-gpt-4o v2\n",
      "Avg Accuracy: 0.354, Avg Cost: 0.23712 Avg Round: 10.16\n",
      "& 0.354 & 10.16 & 0.23712 \n",
      "base-4o-mini v1\n",
      "Avg Accuracy: 0.192, Avg Cost: 0.00922 Avg Round: 12.91\n",
      "& 0.192 & 12.91 & 0.00922 \n",
      "strategy-4o-mini v2\n",
      "Avg Accuracy: 0.29, Avg Cost: 0.01048 Avg Round: 11.221\n",
      "& 0.29 & 11.221 & 0.01048 \n",
      "expel-4o-mini v2\n",
      "Avg Accuracy: 0.311, Avg Cost: 0.02298 Avg Round: 8.43\n",
      "& 0.311 & 8.43 & 0.02298 \n",
      "react-4o-mini v2\n",
      "Avg Accuracy: 0.274, Avg Cost: 0.01551 Avg Round: 9.716\n",
      "& 0.274 & 9.716 & 0.01551 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tab2_one_trial = {\n",
    "  \"base-gpt-4o\": \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\",\n",
    "  \"strategy-gpt-4o\": (\"PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "  \"expel-gpt-4o\": (\"ExpelAgent_gpt-4o_c392_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "  \"react-gpt-4o\": (\"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "\n",
    "  \"base-4o-mini\": \"BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\",\n",
    "  \"strategy-4o-mini\": (\"PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "  \"expel-4o-mini\": (\"ExpelAgent_4o-mini_c393_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "  \"react-4o-mini\": (\"ReactAgent_4o-mini_c131_alert_level_t0_s15_trial1\", \"v2\"),\n",
    "}\n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "\n",
    "for name, b in tab2_one_trial.items():\n",
    "    if type(b) == tuple:\n",
    "        file_folder, version = b\n",
    "    else:\n",
    "        file_folder = b\n",
    "        version = \"v1\"\n",
    "    if \"gpt-4o\" in name:\n",
    "        model_name = \"gpt-4o\"\n",
    "    elif \"4o-mini\" in name:\n",
    "        model_name = \"gpt-4o-mini\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {name}\")\n",
    "    \n",
    "    print(name, version)\n",
    "    avg_accuracy, avg_cost, avg_round = get_avg_accuracy_and_cost(log_path, file_folder, model_name, version=version)\n",
    "    print(f\"Avg Accuracy: {avg_accuracy}, Avg Cost: {avg_cost}\", \n",
    "          f\"Avg Round: {avg_round}\")\n",
    "    print(f\"& {avg_accuracy} & {avg_round} & {avg_cost} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy-gpt-4o v2\n",
      "Avg Accuracy: 0.473, Avg Cost: 0.36471 Avg Round: 26.066\n",
      "& 0.473 & 26.066 & 0.36471 \n",
      "===================\n",
      "strategy-reflect-gpt-4o v2\n",
      "Avg Accuracy: 0.505, Avg Cost: 0.46978 Avg Round: 24.156\n",
      "& 0.505 & 24.156 & 0.46978 \n",
      "===================\n",
      "react-gpt-4o v2\n",
      "Avg Accuracy: 0.563, Avg Cost: 0.48833 Avg Round: 20.233\n",
      "& 0.563 & 20.233 & 0.48833 \n",
      "===================\n",
      "react-reflect-gpt-4o v2\n",
      "Avg Accuracy: 0.563, Avg Cost: 0.49576 Avg Round: 20.1\n",
      "& 0.563 & 20.1 & 0.49576 \n",
      "===================\n",
      "strategy-4o-mini v2\n",
      "Avg Accuracy: 0.418, Avg Cost: 0.02785 Avg Round: 26.316\n",
      "& 0.418 & 26.316 & 0.02785 \n",
      "===================\n",
      "strategy-reflect-4o-mini v2\n",
      "Avg Accuracy: 0.44, Avg Cost: 0.02775 Avg Round: 25.866\n",
      "& 0.44 & 25.866 & 0.02775 \n",
      "===================\n",
      "react-reflexion-4o-mini v2\n",
      "Avg Accuracy: 0.452, Avg Cost: 0.0351 Avg Round: 23.392\n",
      "& 0.452 & 23.392 & 0.0351 \n",
      "===================\n",
      "react-4o-mini v2\n",
      "Avg Accuracy: 0.423, Avg Cost: 0.03567 Avg Round: 23.757\n",
      "& 0.423 & 23.757 & 0.03567 \n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "def analysis_3_trials(data: dict, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    • Aggregates rounds & tokens over every trial.\n",
    "    • For each trial, picks the *last* model in `usage_summary`\n",
    "      when adding prompt/completion tokens.\n",
    "    • Submission / evaluation flags inspected only on the last trial\n",
    "      (per earlier request).\n",
    "    \"\"\"\n",
    "    total_len = len(data)\n",
    "    total_reward = total_round = 0\n",
    "    success_count = non_zero_reward_count = not_submit_count = 0\n",
    "    path_count, reward_count, round_count = {}, {}, {}\n",
    "    eval_error_count = fail_to_run_count = 0\n",
    "    total_prompt_tokens = total_completion_tokens = total_cost = 0\n",
    "    empty_result_count = error_query_count = query_count = 0  # placeholders\n",
    "\n",
    "    for q in data:\n",
    "        p_len = len(q[\"question_dict\"][\"shortest_alert_path\"])\n",
    "        path_count.setdefault(p_len, 0)\n",
    "        reward_count.setdefault(p_len, 0)\n",
    "        round_count.setdefault(p_len, 0)\n",
    "\n",
    "        # reward bookkeeping (once per question)\n",
    "        reward = q[\"reward\"]\n",
    "        total_reward += reward\n",
    "        reward_count[p_len] += reward\n",
    "        if reward > 0:\n",
    "            non_zero_reward_count += 1\n",
    "        if reward == 1:\n",
    "            success_count += 1\n",
    "        path_count[p_len] += 1\n",
    "\n",
    "        # ---- iterate over ALL trials ------------------------------------\n",
    "        for trial in q[\"trials\"].values():\n",
    "            msgs = trial.get(\"messages\", [])\n",
    "            tmp_round = max((len(msgs) - 1) // 2, 0)\n",
    "            total_round += tmp_round\n",
    "            round_count[p_len] += tmp_round\n",
    "\n",
    "            # === NEW token logic: use *last* model =======================\n",
    "            usage_summary = trial.get(\"usage_summary\", {})\n",
    "            if usage_summary:                                   # guard empty dict\n",
    "                try:\n",
    "                    mdl = list(usage_summary.keys())[-1]        # last‑listed model\n",
    "                    total_prompt_tokens     += usage_summary[mdl].get(\"prompt_tokens\", 0)\n",
    "                    total_completion_tokens += usage_summary[mdl].get(\"completion_tokens\", 0)\n",
    "                except Exception as e:\n",
    "                    print(f\"[usage error] q nodes={q['nodes']} trial={mdl}: {e}\")\n",
    "\n",
    "        # ---- submission / evaluation checks on final trial -------------\n",
    "        last_trial = list(q[\"trials\"].values())[-1]\n",
    "        if not last_trial[\"info\"].get(\"submit\"):\n",
    "            not_submit_count += 1\n",
    "        elif not (last_trial[\"info\"].get(\"is_json_success\", True)\n",
    "                  and last_trial[\"info\"].get(\"is_reflect_success\", True)):\n",
    "            print(f\"[eval error] q nodes={q['nodes']}: {last_trial['info']}\")\n",
    "            eval_error_count += 1\n",
    "\n",
    "    # ---- optional console summary --------------------------------------\n",
    "    if verbose:\n",
    "        print(f\"Average reward: {total_reward / total_len:.6f}\")\n",
    "        print(f\"Average round:  {total_round  / total_len:.6f}\")\n",
    "\n",
    "    return {\n",
    "        \"total_len\": total_len,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"success_count\": success_count,\n",
    "        \"non_zero_reward_count\": non_zero_reward_count,\n",
    "        \"not_submit_count\": not_submit_count,\n",
    "        \"total_round\": total_round,\n",
    "        \"total_prompt_tokens\": total_prompt_tokens,\n",
    "        \"total_completion_tokens\": total_completion_tokens,\n",
    "        \"total_cost\": total_cost,   # still placeholder\n",
    "        \"empty_result_count\": empty_result_count,\n",
    "        \"error_query_count\": error_query_count,\n",
    "        \"query_count\": query_count,\n",
    "        \"eval_error_count\": eval_error_count,\n",
    "        \"fail_to_run_count\": fail_to_run_count,\n",
    "    }\n",
    "\n",
    "def get_avg_accuracy_and_cost_3_trial(log_path, file_folder, model_name, version=\"v1\", round_cut=-1):\n",
    "    d = 1_000_000  # price is per 1M tokens\n",
    "    prices = {\n",
    "        \"gpt-4o\": (2.5 / d, 10 / d),\n",
    "        \"gpt-4o-mini\": (0.15 / d, 0.6 / d),\n",
    "        \"o1-mini\": (1.1 / d, 4.4 / d),\n",
    "        \"o1\": (15 / d, 7.5 / d),\n",
    "        \"o3-mini\": (1.1 / d, 4.4 / d),\n",
    "        \"deepseek-r1\": (0.55 / d, 0.27 / d),\n",
    "        \"phi4\": (0.07/ d, 0.14/ d),\n",
    "    }\n",
    "\n",
    "    prompt_price, completion_price = prices[model_name]\n",
    "    file_template = f\"{log_path}/{file_folder}\" + \"/agent_incident_{0}.json\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "    total_round = 0\n",
    "\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "    for i in incidents:\n",
    "        with open(file_template.format(i), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if version == \"v2\":\n",
    "            result = analysis_3_trials(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown version: {}\".format(version))\n",
    "\n",
    "        total_count += result['total_len']\n",
    "        total_reward += result['total_reward']\n",
    "        total_prompt_tokens += result['total_prompt_tokens']\n",
    "        total_completion_tokens += result['total_completion_tokens']\n",
    "        total_round += result['total_round']\n",
    "\n",
    "    avg_accuracy = round(total_reward / total_count, 3)\n",
    "    avg_cost = round(\n",
    "        (total_prompt_tokens * prompt_price + total_completion_tokens * completion_price)\n",
    "        / total_count,\n",
    "        3,\n",
    "    )\n",
    "    avg_round = round(total_round / total_count, 1)\n",
    "\n",
    "    return avg_accuracy, avg_cost, avg_round\n",
    "\n",
    "tab2_3_trial = {\n",
    "  \"strategy-gpt-4o\": (\"PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"strategy-reflect-gpt-4o\": (\"PromptSauceReflexionAgent_gpt-4o_c234_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"react-gpt-4o\": (\"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"react-reflect-gpt-4o\": (\"ReActReflexionAgent_gpt-4o_c378_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "\n",
    "  \"strategy-4o-mini\": (\"PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"strategy-reflect-4o-mini\": (\"PromptSauceReflexionAgent_4o-mini_c234_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"react-reflexion-4o-mini\": (\"ReActReflexionAgent_4o-mini_c378_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "  \"react-4o-mini\": (\"ReactAgent_4o-mini_c124_alert_level_t0_s15_trial3\", \"v2\"),\n",
    "}\n",
    "\n",
    "log_path = \"../secgym/final_results\"\n",
    "for name, b in tab2_3_trial.items():\n",
    "    if type(b) == tuple:\n",
    "        file_folder, version = b\n",
    "    else:\n",
    "        file_folder = b\n",
    "        version = \"v1\"\n",
    "    if \"gpt-4o\" in name:\n",
    "        model_name = \"gpt-4o\"\n",
    "    elif \"4o-mini\" in name:\n",
    "        model_name = \"gpt-4o-mini\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {name}\")\n",
    "    \n",
    "    print(name, version)\n",
    "    avg_accuracy, avg_cost, avg_round = get_avg_accuracy_and_cost_3_trial(log_path, file_folder, model_name, version=version)\n",
    "    print(f\"Avg Accuracy: {avg_accuracy}, Avg Cost: {avg_cost}\", \n",
    "          f\"Avg Round: {avg_round}\")\n",
    "    print(f\"& {avg_accuracy} & {avg_round} & {avg_cost} \")\n",
    "    print(\"===================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\n",
      "589\n",
      "& 0.304 & 0.268 & 0.364 & 0.238 & 0.218 & 0.439 & 0.126 & 0.304 & 0.261 \n",
      "PromptSauceAgent_gpt-4o_c83_alert_level_t0_s15_trial2\n",
      "589\n",
      "& 0.257 & 0.339 & 0.273 & 0.363 & 0.274 & 0.246 & 0.303 & 0.375 & 0.306 \n",
      "ReflexionAgent_gpt-4o_c82_alert_level_t0_s15_trial3\n",
      "589\n",
      "& 0.469 & 0.537 & 0.455 & 0.417 & 0.344 & 0.463 & 0.379 & 0.606 & 0.447 \n",
      "ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\n",
      "589\n",
      "& 0.365 & 0.383 & 0.091 & 0.308 & 0.324 & 0.526 & 0.264 & 0.446 & 0.354 \n",
      "----------------------------------------\n",
      "BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\n",
      "589\n",
      "& 0.132 & 0.188 & 0.273 & 0.169 & 0.164 & 0.218 & 0.12 & 0.257 & 0.172 \n",
      "PromptSauceAgent_4o-mini_c79_alert_level_t0_s15_trial2\n",
      "589\n",
      "& 0.294 & 0.39 & 0.182 & 0.196 & 0.185 & 0.193 & 0.246 & 0.268 & 0.251 \n",
      "ReflexionAgent_4o-mini_c80_alert_level_t0_s15_trial3\n",
      "589\n",
      "& 0.433 & 0.488 & 0.182 & 0.459 & 0.26 & 0.491 & 0.492 & 0.539 & 0.435 \n"
     ]
    }
   ],
   "source": [
    "tab2_4o = [\n",
    "    \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\",\n",
    "    \"PromptSauceAgent_gpt-4o_c83_alert_level_t0_s15_trial2\",\n",
    "    \"ReflexionAgent_gpt-4o_c82_alert_level_t0_s15_trial3\",\n",
    "    \"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\"\n",
    "\n",
    "]\n",
    "\n",
    "tab2_4o_mini = [\n",
    "    \"BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\",\n",
    "    \"PromptSauceAgent_4o-mini_c79_alert_level_t0_s15_trial2\",\n",
    "    \"ReflexionAgent_4o-mini_c80_alert_level_t0_s15_trial3\"\n",
    "]\n",
    "\n",
    "for b in tab2_4o:\n",
    "    print(b)\n",
    "    get_over_leaf_format(log_path, b, round_cut=15)\n",
    "\n",
    "print(\"-\"*40)\n",
    "for b in tab2_4o_mini:\n",
    "    print(b)\n",
    "    get_over_leaf_format(log_path, b, round_cut=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base+Strategy Prompt\n",
      "589\n",
      "& 0.324 & 0.334 & 0.455 & 0.298 & 0.224 & 0.27 & 0.257 & 0.311 & 0.29 \n",
      "ReAct\n",
      "589\n",
      "& 0.365 & 0.383 & 0.091 & 0.308 & 0.324 & 0.526 & 0.264 & 0.446 & 0.354 \n",
      "Base+Strategy Prompt x3\n",
      "589\n",
      "& 0.457 & 0.515 & 0.364 & 0.469 & 0.414 & 0.526 & 0.46 & 0.543 & 0.473 \n",
      "ReAct x3\n",
      "589\n",
      "& 0.614 & 0.554 & 0.455 & 0.473 & 0.554 & 0.737 & 0.517 & 0.574 & 0.563 \n",
      "ReAct+Reflexion\n",
      "No usage summary, skipping 82-82, trial 1\n",
      "No usage summary, skipping 82-82, trial 2\n",
      "No usage summary, skipping 156-138, trial 0\n",
      "No usage summary, skipping 156-138, trial 1\n",
      "No usage summary, skipping 156-138, trial 2\n",
      "589\n",
      "& 0.581 & 0.573 & 0.818 & 0.449 & 0.53 & 0.737 & 0.538 & 0.585 & 0.563 \n",
      "Base+Strategy Prompt+Reflexion\n",
      "No usage summary, skipping 137-136, trial 0\n",
      "589\n",
      "& 0.48 & 0.524 & 0.545 & 0.491 & 0.474 & 0.579 & 0.483 & 0.554 & 0.505 \n",
      "Expel\n",
      "589\n",
      "& 0.371 & 0.341 & 0.182 & 0.293 & 0.26 & 0.298 & 0.269 & 0.382 & 0.311 \n",
      "----------------------------------------\n",
      "Base+Strategy Prompt\n",
      "No usage summary, skipping 120-120, trial 0\n",
      "589\n",
      "& 0.263 & 0.371 & 0.273 & 0.239 & 0.201 & 0.277 & 0.28 & 0.325 & 0.273 \n",
      "ReAct\n",
      "589\n",
      "& 0.278 & 0.268 & 0.545 & 0.165 & 0.244 & 0.333 & 0.287 & 0.382 & 0.274 \n",
      "Base+Strategy Prompt x3\n",
      "No usage summary, skipping 148-75, trial 1\n",
      "589\n",
      "& 0.411 & 0.485 & 0.636 & 0.355 & 0.334 & 0.439 & 0.402 & 0.556 & 0.418 \n",
      "ReAct x3\n",
      "589\n",
      "& 0.402 & 0.444 & 0.636 & 0.347 & 0.396 & 0.456 & 0.402 & 0.568 & 0.423 \n",
      "ReAct+Reflexion\n",
      "589\n",
      "& 0.408 & 0.505 & 0.545 & 0.367 & 0.48 & 0.526 & 0.414 & 0.511 & 0.452 \n",
      "Base+Strategy Prompt+Reflexion\n",
      "589\n",
      "& 0.483 & 0.48 & 0.364 & 0.436 & 0.314 & 0.456 & 0.425 & 0.561 & 0.44 \n",
      "Expel\n",
      "589\n",
      "& 0.461 & 0.395 & 0.309 & 0.363 & 0.324 & 0.456 & 0.372 & 0.4 & 0.39 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial1\n",
    "# PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial3\n",
    "# PromptSauceReflexionAgent_gpt-4o_c234_alert_level_t0_s15_trial3\n",
    "# ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\n",
    "# ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial3\n",
    "# ReActReflexionAgent_gpt-4o_c378_alert_level_t0_s15_trial3\n",
    "tab2_4o = {\n",
    "    \"Base+Strategy Prompt\": \"PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial1\",\n",
    "    \"ReAct\": \"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\",\n",
    "    \"Base+Strategy Prompt x3\": \"PromptSauceAgent_gpt-4o_c345_alert_level_t0_s15_trial3\",\n",
    "    \"ReAct x3\": \"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial3\",\n",
    "    \"ReAct+Reflexion\": \"ReActReflexionAgent_gpt-4o_c378_alert_level_t0_s15_trial3\",\n",
    "    \"Base+Strategy Prompt+Reflexion\": \"PromptSauceReflexionAgent_gpt-4o_c234_alert_level_t0_s15_trial3\",\n",
    "    \"Expel\" : \"ExpelAgent_4o-mini_c393_alert_level_t0_s15_trial1\"\n",
    "}\n",
    "\n",
    "# [\n",
    "#     \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\",\n",
    "#     \"PromptSauceAgent_gpt-4o_c83_alert_level_t0_s15_trial2\",\n",
    "#     \"ReflexionAgent_gpt-4o_c82_alert_level_t0_s15_trial3\",\n",
    "#     \"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\"\n",
    "# ]\n",
    "\n",
    "# PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial1\n",
    "# PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial3\n",
    "# ReactAgent_4o-mini_c124_alert_level_t0_s15_trial3\n",
    "# ReactAgent_4o-mini_c131_alert_level_t0_s15_trial1\n",
    "# ReActReflexionAgent_4o-mini_c378_alert_level_t0_s15_trial3\n",
    "# PromptSauceReflexionAgent_4o-mini_c234_alert_level_t0_s15_trial3\n",
    "\n",
    "tab2_4o_mini = {\n",
    "    \"Base+Strategy Prompt\": \"PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial1\",\n",
    "    \"ReAct\": \"ReactAgent_4o-mini_c131_alert_level_t0_s15_trial1\",\n",
    "    \"Base+Strategy Prompt x3\": \"PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial3\",\n",
    "    \"ReAct x3\": \"ReactAgent_4o-mini_c124_alert_level_t0_s15_trial3\",\n",
    "    \"ReAct+Reflexion\": \"ReActReflexionAgent_4o-mini_c378_alert_level_t0_s15_trial3\",\n",
    "    \"Base+Strategy Prompt+Reflexion\": \"PromptSauceReflexionAgent_4o-mini_c234_alert_level_t0_s15_trial3\",\n",
    "    \"Expel\" : \"ExpelAgent_gpt-4o_c392_alert_level_t0_s15_trial1\"\n",
    "}\n",
    "\n",
    "log_path = \"../secgym/final_results\"    \n",
    "\n",
    "# get_over_leaf_format(log_path, \"PromptSauceAgent_4o-mini_c34_alert_level_t0_s15_trial1\")\n",
    "\n",
    "for k, v in tab2_4o.items():\n",
    "    print(k)\n",
    "    get_over_leaf_format(log_path, v, round_cut=15)\n",
    "\n",
    "print(\"-\"*40)\n",
    "for k, v in tab2_4o_mini.items():\n",
    "    print(k)\n",
    "    get_over_leaf_format(log_path, v, round_cut=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine trials to get Best of 3 trials for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.359 & 0.51 & 0.4 & 0.301 & 0.237 & 0.333 & 0.366 & 0.382 & 0.351 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def combine_results(t2step15_data, t1step25data, round_cut=-1):\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range(len(t2step15_data)):\n",
    "        d1 = t2step15_data[i]\n",
    "        d2 = t1step25data[i]\n",
    "        if d1.get(\"usage_summary\") is None or d2.get(\"usage_summary\") is None:\n",
    "            print(f\"Missing usage summary for {i}\")\n",
    "            continue\n",
    "        if round_cut != -1 and (len(d2[\"messages\"]) - 1) // 2 > round_cut:\n",
    "            # print(f\"Cutting off at round {round_cut}\")\n",
    "            d2['reward'] = 0\n",
    "        total_reward += max(d1['reward'], d2['reward']) # best of 3\n",
    "        total_count += 1\n",
    "\n",
    "    return {\n",
    "        \"total_len\": total_count,\n",
    "        \"total_reward\": total_reward\n",
    "    }\n",
    "\n",
    "\n",
    "def get_over_leaf_format_for_2(log_path, t2step15_file, t1step25_file, round_cut=-1):\n",
    "    t2step15_file_template = f\"{log_path}/{t2step15_file}\" + \"/agent_incident_{0}.json\"\n",
    "    t1step25_file_template = f\"{log_path}/{t1step25_file}\" + \"/agent_incident_{0}.json\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "    accs_str = \"\"\n",
    "\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "    for i in incidents:\n",
    "        # print(f\"Analysis for incident {i}\")\n",
    "        with open(t2step15_file_template.format(i), \"r\") as f:\n",
    "            data1 = json.load(f)\n",
    "        with open(t1step25_file_template.format(i), \"r\") as f:\n",
    "            data2 = json.load(f)\n",
    "\n",
    "        result = combine_results(data1, data2, round_cut=round_cut)\n",
    "        accs_str += \"& \" + str(round(result['total_reward']/result['total_len'], 3)) + \" \"\n",
    "\n",
    "        total_count += result['total_len']\n",
    "        total_reward += result['total_reward']\n",
    "    \n",
    "    accs_str += \"& \" + str(round(total_reward/total_count, 3)) + \" \"\n",
    "    print(accs_str)\n",
    "\n",
    "\n",
    "get_over_leaf_format_for_2(log_path, \n",
    "                           \"PromptSauceAgent_4o-mini_c79_alert_level_t0_s15_trial2\",\n",
    "                           \"PromptSauceAgent_4o-mini_c73_alert_level_t0_s25_trial1\",\n",
    "                           round_cut=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.353 & 0.48 & 0.364 & 0.455 & 0.348 & 0.368 & 0.407 & 0.471 & 0.408 \n"
     ]
    }
   ],
   "source": [
    "get_over_leaf_format_for_2(log_path, \n",
    "                           \"PromptSauceAgent_gpt-4o_c83_alert_level_t0_s15_trial2\",\n",
    "                           \"PromptSauceAgent_gpt-4o_c72_alert_level_t0_s25_trial1\",\n",
    "                           round_cut=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o and GPT-4o-mini with responding to steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.02 & 0.024 & 0.083 & 0.02 & 0.014 & 0.017 & 0.023 & 0.035 & 0.022 & 0.009 \n",
      "& 0.074 & 0.174 & 0.083 & 0.12 & 0.074 & 0.138 & 0.101 & 0.236 & 0.122 & 0.009 \n",
      "& 0.144 & 0.198 & 0.25 & 0.157 & 0.144 & 0.224 & 0.129 & 0.323 & 0.179 & 0.009 \n",
      "& 0.194 & 0.209 & 0.25 & 0.181 & 0.184 & 0.241 & 0.179 & 0.376 & 0.213 & 0.009 \n",
      "& 0.194 & 0.209 & 0.25 & 0.197 & 0.184 & 0.241 & 0.179 & 0.386 & 0.217 & 0.009 \n"
     ]
    }
   ],
   "source": [
    "gpt4omini_results = []\n",
    "for r in [5, 10, 15, 20, 25]:\n",
    "    avg_reward = get_over_leaf_format(log_path, \"BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\", round_cut=r)\n",
    "    gpt4omini_results.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.1 & 0.129 & 0.0 & 0.112 & 0.03 & 0.069 & 0.034 & 0.105 & 0.08 & 0.464 \n",
      "& 0.202 & 0.24 & 0.083 & 0.182 & 0.182 & 0.31 & 0.103 & 0.228 & 0.197 & 0.464 \n",
      "& 0.312 & 0.311 & 0.25 & 0.233 & 0.212 & 0.414 & 0.126 & 0.298 & 0.262 & 0.464 \n",
      "& 0.35 & 0.311 & 0.25 & 0.243 & 0.212 & 0.483 & 0.161 & 0.333 & 0.285 & 0.464 \n",
      "& 0.37 & 0.334 & 0.25 & 0.273 & 0.215 & 0.483 & 0.172 & 0.333 & 0.299 & 0.464 \n"
     ]
    }
   ],
   "source": [
    "gpt40_results = []\n",
    "for r in [5, 10, 15, 20, 25]:\n",
    "    avg_reward = get_over_leaf_format(log_path, \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\", round_cut=r)\n",
    "    gpt40_results.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_turns_vs_reward(model_rewards: dict, steps: list, output_filename: str):\n",
    "    \"\"\"\n",
    "    Plots the relation between the number of interaction turns and rewards for different models.\n",
    "\n",
    "    Parameters:\n",
    "    - model_rewards (dict): A dictionary where keys are model names and values are lists of rewards.\n",
    "    - steps (list): A list of interaction turns (e.g., [5, 10, 15, 20, 25]).\n",
    "    - output_filename (str): The filename for saving the output plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7, 5), dpi=300)  # High-resolution figure\n",
    "\n",
    "    markers = ['o', 's', 'D', '^', 'v', '*', 'P', 'X']  # Different markers for distinction\n",
    "    linestyles = ['-', '--', '-.', ':']  # Variety of line styles\n",
    "    colors = plt.get_cmap(\"tab10\").colors  # Professional color palette\n",
    "\n",
    "    for i, (model, rewards) in enumerate(model_rewards.items()):\n",
    "        plt.plot(\n",
    "            steps,\n",
    "            rewards,\n",
    "            marker=markers[i % len(markers)], \n",
    "            linestyle=linestyles[i % len(linestyles)], \n",
    "            color=colors[i % len(colors)],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=model\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Number of Interaction Turns\", fontsize=18)\n",
    "    plt.ylabel(\"Reward\", fontsize=18)\n",
    "    plt.xticks(steps, fontsize=15)  # Explicitly set x-ticks to given steps\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.legend(fontsize=17, loc=\"best\", frameon=True)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename, format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "# Example usage:\n",
    "model_rewards = {\n",
    "    \"GPT-4o-mini\": gpt4omini_results,\n",
    "    \"GPT-4o\": gpt40_results\n",
    "}\n",
    "steps = [5, 10, 15, 20, 25]\n",
    "plot_turns_vs_reward(model_rewards, steps, \"reward_vs_turns.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward with respect to steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_over_leaf_format_path_wise(log_path, file_folder, round_cut=-1):\n",
    "    file_template = f\"{log_path}/{file_folder}\" + \"/agent_incident_{0}.json\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_reward = 0\n",
    "    total_success_count = 0\n",
    "    total_cost = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "\n",
    "    accs_str = \"\"\n",
    "\n",
    "    data_by_path_count = {}\n",
    "    incidents = [5, 34, 38, 39, 55, 134, 166, 322]\n",
    "    for i in incidents:\n",
    "        # print(f\"Analysis for incident {i}\")\n",
    "        with open(file_template.format(i), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        for d in data:\n",
    "            p = len(d['question_dict']['shortest_alert_path'])\n",
    "            if p not in data_by_path_count:\n",
    "                data_by_path_count[p] = []\n",
    "            data_by_path_count[p].append(d)\n",
    "\n",
    "    keys = list(data_by_path_count.keys())\n",
    "    keys.sort()\n",
    "\n",
    "    count_str = \"\"\n",
    "    for p in keys:\n",
    "        data = data_by_path_count[p]\n",
    "        count_str += f\"& {len(data)} \"\n",
    "        result = analysis(data, False, round_cut=round_cut)\n",
    "        accs_str += \"& \" + str(round(result['total_reward']/result['total_len'], 3)) + \" \"\n",
    "        total_count += result['total_len']\n",
    "        total_reward += result['total_reward']\n",
    "        total_success_count += result['success_count']\n",
    "        total_cost += result['total_cost']\n",
    "        total_prompt_tokens += result['total_prompt_tokens']\n",
    "        total_completion_tokens += result['total_completion_tokens']\n",
    "    \n",
    "    accs_str += \"& \" + str(round(total_reward/total_count, 3)) + \" \"\n",
    "    accs_str += \"& \" + str(round(total_cost/total_count, 3)) + \" \"\n",
    "\n",
    "    pl_str = \"\"\n",
    "    for k in keys:\n",
    "        pl_str += f\"& {k} \"\n",
    "    \n",
    "    print(pl_str)\n",
    "    print(count_str)    \n",
    "\n",
    "    print(accs_str)\n",
    "    return round(total_reward/total_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions removed: 0\n",
      "Filtered files saved to: /Users/kevin/Downloads/SecRL/secgym/env/questions/min_overlap/test_filtered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Define base directories\n",
    "base = \"/Users/kevin/Downloads/SecRL/secgym/env/questions/min_overlap/test\"\n",
    "new_base = \"/Users/kevin/Downloads/SecRL/secgym/env/questions/min_overlap/test_filtered\"\n",
    "\n",
    "# Ensure new folder exists\n",
    "os.makedirs(new_base, exist_ok=True)\n",
    "\n",
    "removed_count = 0\n",
    "\n",
    "for file in os.listdir(base):\n",
    "    if file.endswith(\".json\"):\n",
    "        file_path = os.path.join(base, file)\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Filter out questions with context < 50\n",
    "        filtered_data = [q for i, q in enumerate(data) if len(q['context']) >= 50]\n",
    "\n",
    "        # Remove question 9 if it's incident_9.json\n",
    "        if file == \"incident_9.json\" and len(filtered_data) > 9:\n",
    "            del filtered_data[9]  # Question 9 is index 9\n",
    "\n",
    "        removed_count += len(data) - len(filtered_data)\n",
    "\n",
    "        # Save to new folder\n",
    "        new_file_path = os.path.join(new_base, file)\n",
    "        with open(new_file_path, \"w\") as f:\n",
    "            json.dump(filtered_data, f, indent=4)\n",
    "\n",
    "print(f\"Total questions removed: {removed_count}\")\n",
    "print(f\"Filtered files saved to: {new_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 1 & 3 & 5 & 7 & 9 \n",
      "& 46 & 423 & 98 & 31 & 1 \n",
      "& 0.391 & 0.217 & 0.158 & 0.117 & 1.0 & 0.217 & 0.009 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.217"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_over_leaf_format_path_wise(log_path, \"BaselineAgent_4o-mini_c71_alert_level_t0_s25_trial1\", round_cut=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 1 & 3 & 5 & 7 & 9 \n",
      "& 46 & 423 & 98 & 31 & 1 \n",
      "& 0.304 & 0.294 & 0.284 & 0.392 & 1.0 & 0.299 & 0.464 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.299"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_over_leaf_format_path_wise(log_path, \"BaselineAgent_gpt-4o_c70_alert_level_t0_s25_trial1\", round_cut=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 1 & 3 & 5 & 7 & 9 \n",
      "& 46 & 423 & 98 & 31 & 1 \n",
      "& 0.217 & 0.253 & 0.228 & 0.196 & 1.0 & 0.244 & 0.016 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.244"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_over_leaf_format_path_wise(log_path, \"PromptSauceAgent_4o-mini_c73_alert_level_t0_s25_trial1\", round_cut=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: use round_cut with trial data\n",
      "Warning: use round_cut with trial data\n",
      "Warning: use round_cut with trial data\n",
      "Warning: use round_cut with trial data\n",
      "Warning: use round_cut with trial data\n",
      "& 1 & 3 & 5 & 7 & 9 \n",
      "& 46 & 423 & 98 & 31 & 1 \n",
      "& 0.37 & 0.261 & 0.196 & 0.228 & 1.0 & 0.258 & 0.02 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.258"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_over_leaf_format_path_wise(log_path, \"PromptSauceAgent_4o-mini_c79_alert_level_t0_s15_trial2\", round_cut=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_over_leaf_format_path_wise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_over_leaf_format_path_wise\u001b[49m(log_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\u001b[39m\u001b[38;5;124m\"\u001b[39m, round_cut\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_over_leaf_format_path_wise' is not defined"
     ]
    }
   ],
   "source": [
    "get_over_leaf_format_path_wise(log_path, \"ReactAgent_gpt-4o_c121_alert_level_t0_s15_trial1\", round_cut=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
